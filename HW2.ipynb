{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 2\n",
    "Created by Prof. [Mohammad M. Ghassemi](https://ghassemi.xyz)\n",
    "\n",
    "Submitted by: <span style=\"color:red\"> INSERT YOUR NAME HERE </span>\n",
    "\n",
    "In collaboration with: <span style=\"color:red\"> INSERT YOUR (OPTIONAL) HOMEWORK PARTNER'S NAME HERE </span>\n",
    "\n",
    "\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Goals\n",
    "The goal of this assignment is to familiarize yourself with:\n",
    "\n",
    "1. Parsing HTML data\n",
    "2. Text classification using ngram language models\n",
    "3. Text classification using supervised machine learning algorithms\n",
    "4. Tools for sentiment analysis\n",
    "\n",
    "The assignment combines tutorial components, with learning exercises that you must complete and submit. The learning exercise sections are clearly demarcated within the assignments.\n",
    "\n",
    "## Before you start\n",
    "1. PULL THE LATEST VERSION OF THE `course-materials` REPOSITORY, AND COPY `homework/HW2/` INTO THE CORRESPONDING DIRECTORY OF YOUR SUBMISSION FOLDER\n",
    "2. CREATE AND ATTACH TO A VIRTUAL ENVIRONMENT, AND INSTALL THE REQUIREMENTS IN `requirements.txt`\n",
    "3. IMPORT THE COURSE UTILITIES BY RUNNING THE CODE BLOCK BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ghamut/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/ghamut/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'materials.code.utils' from '/Users/ghamut/Documents/course-materials/homework/HW2/materials/code/utils.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from materials.code import utils\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Part 0: Collecting Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, we will continue our journey through the text of the philosophers. More specifically, we'll be exploring how to use a natural language data for classification problems. To begin, let's return back to [Project Gutenburg](https://www.gutenberg.org/) and notice that the website not only provides us with access to the text of Bertrand's Russel's The Problem's of Philosophy, but also provides us with some interesting meta-data in the `Bibliographic Record` shown in the screen-shot below:\n",
    "\n",
    "![Figure1](materials/images/Figure1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Bibliographic record provided by Project Gutenberg provides **classifications** of the books according their Library of Congress Class (`LOC Class`), as well as a `Subject`. As we discussed in the lectures, classifications are useful because they take our messy, continuous world and break it into manageable groupings that we can more easily act upon. Thanks to Project Gutenburg's classification, we can see that `The Problems of Philosophy` is a book about `Philosophy` (who would have guessed?!) and by knowing this classification we may (and most likely will) make some simplifying assumptions about the book without reading even a single line of it. \n",
    "\n",
    "It would be useful if we could collect this Bibliographic record, in addition to the information in the raw text itself. But in order for us to extract that information, we'll first need to obtain some practice processing, and extracting information from, HTML documents. If this is your first time looking at HTML documents and you would like to review how they are formatted, you can use [W3 Schools](https://www.w3schools.com/html/default.asp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting and processing HTML data\n",
    "Let's start by looking through some simple HTML documents for a mythical social network that I've stored locally in this respository (`materials/html/`): [Anqa.html](materials/html/Anqa.html), [Garuda.html](materials/html/Garuda.html), [Konrul.html](materials/html/Konrul.html), [Nue.html](materials/html/Nue.html) and [Simargl.html](materials/html/Simargl.html). You will notice that each HTML file simply contains a list of friends and enemies with links to other webpages. Here's what `Anqa.html` contains:\n",
    "\n",
    "![Figure3](materials/images/Figure3.png)\n",
    "\n",
    "And here is what gets displayed by the browser, using the HMTL file.\n",
    "\n",
    "![Figure2](materials/images/Figure2.png)\n",
    "\n",
    "To help us obtain some familiarity with parsing these types of documents, let's see if we can write some code to rank order these five mysterious figures based on the number of friends and enemies that they have mentioned in their web pages. The first step in that process will be to read the HTML pages into Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A set of local HTML files\n",
    "html_files = ['Anqa.html','Garuda.html','Konrul.html','Nue.html','Simargl.html']\n",
    "\n",
    "# Read each file into a dictionary of HTML files\n",
    "html = {}\n",
    "for file in html_files:\n",
    "    f = open('materials/html/' + file, 'r'); \n",
    "    html[file] = content = f.read(); \n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>We now have a dictionary containing each of the HTML files. Here's Anqa.html:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<html>\n",
      "<head><title>Anqa's Page</title></head>\n",
      "<body>\n",
      "\n",
      "<p class=\"friend\"> My friends are:\n",
      "<a href=\"/lab/tree/materials/html/Konrul.html\" class=\"best\"  id=\"link1\">Konrul</a> and\n",
      "</p>\n",
      "    \n",
      "<p class=\"enemy\"> My enemies are:\n",
      "<a href=\"/lab/tree/materials/html/Nue.html\"  class=\"worst\" id=\"link1\">Nue</a>.\n",
      "</p>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print(html['Anqa.html'])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> The trick to extracting information from HTML documents is to look for a unified structure across documents that your parser can take advantage of. If we inspect the raw HTML of all five documents, we'll notice that they each contain a `<p class=\"friend\">...</p>` and a `<p class=\"enemy\">...</p>` section and that within those sections there are multiple `<a>` tags that list out the names, and links to the pages of, friends and enemies. To extract this information we can use some carefully crafted Regular Expressions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{'Anqa': {'enemies': ['Nue'], 'friends': ['Konrul']},\n",
      " 'Garuda': {'enemies': ['Konrul'], 'friends': ['Anqa', 'Konrul']},\n",
      " 'Konrul': {'enemies': ['Anqa'], 'friends': ['Nue']},\n",
      " 'Nue': {'enemies': [], 'friends': ['Simargl', 'Nue']},\n",
      " 'Simargl': {'enemies': ['Nue'], 'friends': []}}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint \n",
    "\n",
    "# Function to extract the friendship data from an HTML page \n",
    "def extractFriendData(html):\n",
    "    _data = {}\n",
    "    \n",
    "    # Extract the friend and the enemy paragraphs:\n",
    "    p_friend = re.findall(r'<p[^>]*class=\"friend\"[^>]*>.*?</p>', html, re.DOTALL)[0]\n",
    "    p_enemy  = re.findall(r'<p[^>]*class=\"enemy\"[^>]*>.*?</p>' , html, re.DOTALL)[0]\n",
    "\n",
    "    # Within each paragraph, find the href\n",
    "    _data['friends'] = re.findall(r'>(Nue|Konrul|Garuda|Simargl|Anqa)<', p_friend, re.DOTALL)\n",
    "    _data['enemies'] = re.findall(r'>(Nue|Konrul|Garuda|Simargl|Anqa)<', p_enemy, re.DOTALL)\n",
    "    \n",
    "    return _data\n",
    "\n",
    "# Run the function for each page, and store results in a dictionary\n",
    "data = {}\n",
    "for page in html.keys():\n",
    "    data[page.split('.')[0]] = extractFriendData(html[page])\n",
    "\n",
    "print('\\n')\n",
    "pprint(data)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Using the code block above, we are able to extract the structured data from the set of HTML documents. From here, it won't take much work to understand who has the most friends, and who has the most enemies in our mythical social network: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friendship counts:\n",
      "{'Anqa': 1, 'Konrul': 2, 'Nue': 2, 'Simargl': 1}\n",
      "\n",
      "\n",
      "Enemy counts:\n",
      "{'Anqa': 1, 'Konrul': 1, 'Nue': 2}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "friends, enemies = [], []\n",
    "for page in data:\n",
    "    friends += data[page]['friends']\n",
    "    enemies += data[page]['enemies']\n",
    "\n",
    "print('Friendship counts:'); pprint(utils.CountFrequency(friends)); print('\\n')\n",
    "print('Enemy counts:'); pprint(utils.CountFrequency(enemies)); print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> The above example is meant to highlight the flexibility and utility of Regular expressions, but regular expressions are not the only way to parse HTML documents in Python. One very popular tool that was designed for this purpose is [Python's `BeautifulSoup` library](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#). To help us understand how BeautifulSoup works, let's build a tool to extract the `Bibliographic Record` information table from Project Guttenburg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Gets a book and meta-data from Project Guttenburg and saves the results to disk:\n",
    "def getGutenburgBooks(urls, savedir='materials/data/', sleep_time = 2):\n",
    "    for url in urls:\n",
    "\n",
    "        time.sleep(sleep_time/2)\n",
    "        book = {}\n",
    "         \n",
    "        #-------------------------------------------------------------------------\n",
    "        # Get raw HTML data:\n",
    "        #-------------------------------------------------------------------------\n",
    "        html   = requests.get(url).text                         # Get the raw HTML  \n",
    "        soup   = BeautifulSoup(html,features=\"html.parser\")     # Format the raw html\n",
    "        files  = soup.find(\"table\", attrs={\"class\": \"files\"})   # Find the <table> in the html with class='files'\n",
    "        bibrec = soup.find(\"table\", attrs={\"class\": \"bibrec\"})  # Find the <table> in the HTML with class='bibrec'                \n",
    "\n",
    "        #-------------------------------------------------------------------------\n",
    "        # Get the text of the book:\n",
    "        #-------------------------------------------------------------------------\n",
    "        time.sleep(sleep_time/2)\n",
    "        text_url = ['https://www.gutenberg.org' + record.get(\"href\") for record in files.find_all(\"a\") if ('.txt' in record.get(\"href\") and 'zip' not in record.get(\"href\"))][0]\n",
    "        book['text'] = requests.get(text_url).content.decode(\"utf-8\", \"strict\")\n",
    "\n",
    "        #-------------------------------------------------------------------------\n",
    "        # Parse the HTML to generate biblographic record:\n",
    "        #-------------------------------------------------------------------------\n",
    "        for record in bibrec.find_all(\"tr\"):\n",
    "            key   = record.th.text.replace('\\n', ' ').strip()\n",
    "            value = record.td.text.replace('\\n', ' ').strip()\n",
    "            if key in book:\n",
    "                book[key] += [value]\n",
    "            else:\n",
    "                book[key] = [value]\n",
    "        \n",
    "        #-------------------------------------------------------------------------\n",
    "        # Save the book to disk\n",
    "        #-------------------------------------------------------------------------\n",
    "        print('Saving ' + book['Title'][0])\n",
    "        x = json.dumps(book)\n",
    "        f    = open(savedir + book['Title'][0].replace(' ','_') + \".json\",\"w\")\n",
    "        f.write(x)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>As shown above, the BeautifulSoup library takes in a `html` file, and creates `soup`: an object that comes built in with several useful features that allow us to parse the HTML more easily including a `.find` function. In our case, we can use `.find` to extract the two HTLM `tables` that contain a link to the text file (`class=\"files\"`) and the bibliographic record (`class=\"bibrec\"`). Once we've extracted these tables, we can parse them to collect both the table, as well as the meta-data!\n",
    "\n",
    "Let's run the tool to collect ten books from Project Gutenburg: 5 from Bertrand Russel, and 5 from Friedrich Nietzsche. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Free Thought and Official Propaganda\n",
      "Saving Mysticism and Logic and Other Essays\n",
      "Saving The Analysis of Mind\n",
      "Saving Proposed Roads to Freedom\n",
      "Saving Why Men Fight: A method of abolishing the international duel\n",
      "Saving Thus Spake Zarathustra: A Book for All and None\n",
      "Saving Beyond Good and Evil\n",
      "Saving The Antichrist\n",
      "Saving Human, All Too Human: A Book for Free Spirits\n",
      "Saving On the Future of our Educational Institutions\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "urls          = [ 'https://www.gutenberg.org/ebooks/44932','https://www.gutenberg.org/ebooks/25447',\n",
    "                  'https://www.gutenberg.org/ebooks/2529','https://www.gutenberg.org/ebooks/690',\n",
    "                  'https://www.gutenberg.org/ebooks/55610','https://www.gutenberg.org/ebooks/1998',\n",
    "                  'https://www.gutenberg.org/ebooks/4363','https://www.gutenberg.org/ebooks/19322',\n",
    "                  'https://www.gutenberg.org/ebooks/38145','https://www.gutenberg.org/ebooks/28146']\n",
    "\n",
    "getGutenburgBooks(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> The function saves all data in `json` format to `materials/data/`. Let's take a peak at the data in `On_the_Future_of_our_Educational_Institutions.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Subject   - On the Future of our Educational Institutions\n",
      "By        - Nietzsche, Friedrich Wilhelm, 1844-1900\n",
      "LoC Class - LB: Education: Theory and practice of education\n",
      "------------------------------------------------------------\n",
      "**Sample Text**:e especially against that flattering illusion\n",
      "that our conditions should be regarded as the standard for all others\n",
      "and as surpassing them. Let it suffice that they are our institutions,\n",
      "that they have not become a part of ourselves by mere accident, and\n",
      "were not laid upon us like a garment; but that they are living\n",
      "monuments of important steps in the progress of civilisation, in some\n",
      "respects even the furniture of a bygone age, and as such link us with\n",
      "the past of our people, and are...\n"
     ]
    }
   ],
   "source": [
    "# Open the book \n",
    "with open('materials/data/On_the_Future_of_our_Educational_Institutions.json') as f:\n",
    "    book = json.load(f)\n",
    "\n",
    "print('------------------------------------------------------------')    \n",
    "print('Subject   - '   + ''.join(book['Title'])  + \n",
    "      '\\nBy        - ' + ''.join(book['Author']) + \n",
    "      '\\nLoC Class - ' + ';'.join(book['LoC Class']))\n",
    "print('------------------------------------------------------------') \n",
    "print('**Sample Text**:' + book['text'][11012:11508] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> Now let's separate the data by author into two corpora, one for `Nietzsche` and one for `Russel` and perform some basic cleaning on the text by removing non-ascii characters, and converting everything to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import json\n",
    "# -----------------------------------------------\n",
    "# Import the books\n",
    "# -----------------------------------------------\n",
    "books = listdir('materials/data/')\n",
    "files = ['materials/data/' + book for book in books if book[0] != '.']\n",
    "\n",
    "data = []\n",
    "for file in files: \n",
    "    with open(file) as f:\n",
    "        x = json.load(f)\n",
    "        data.append(x)\n",
    "# -----------------------------------------------\n",
    "# Merge the text from Nietzsche and Russel into two corpora\n",
    "# -----------------------------------------------\n",
    "Nietzsche, Russel = '', ''\n",
    "for item in data:\n",
    "    if 'Nietzsche' in item['Author'][0]:\n",
    "        Nietzsche  += item['text'].replace(u'\\xa0', u' ').replace(u'\\ufeff', u' ').replace('_', ' ') + ' '\n",
    "    if 'Russel'    in item['Author'][0]:\n",
    "        Russel     += item['text'].replace(u'\\xa0', u' ').replace(u'\\ufeff', u' ').replace('_', ' ') + ' '\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Keeping only the ascii charaacters\n",
    "# -----------------------------------------------\n",
    "Nietzsche = re.sub(r'[^\\x00-\\x7F]+','', Nietzsche)\n",
    "Russel    = re.sub(r'[^\\x00-\\x7F]+','', Russel)  \n",
    "    \n",
    "# -----------------------------------------------\n",
    "# Converting all words to lower case\n",
    "# -----------------------------------------------    \n",
    "Nietzsche  = Nietzsche.lower() \n",
    "Russel     = Russel.lower()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Part 1: Naive classification using ngram language models\n",
    "Now that we have our text data imported and organized, let's try building a very simple author classification model; that is, we want to build a model that can predict the author of mysterious new sentences we have never seen before.  \n",
    "\n",
    "Recall that in the last assignment, you trained two very simple `tri-gram` language models using the works of `Nietzsche` and `Russel`. As you saw, language models estimate the probability of a given ngram, or sequence of ngrams, according to the properties of the training data. Let's start this section of the tutorial by introducing a slightly more enhanced version of the `basicLanguageModel` that we covered in the last tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------------\n",
      "Langugage model structure for data: \"testing this testing this out testing testing\"\n",
      "-----------------------------------------------------------------------------------------------------------\n",
      "{'___TOTALTOKENS___': 7,\n",
      " '___UNIQUETOKENS___': 3,\n",
      " 'out': {'___BLANKUNITPROB___': 0.33333333333333337,\n",
      "         '___NUMBLANKGRAMS___': 2,\n",
      "         '___PRIOR___': 0.14285714285714285,\n",
      "         'testing': 0.3333333333333333},\n",
      " 'testing': {'___BLANKUNITPROB___': 0.25,\n",
      "             '___NUMBLANKGRAMS___': 1,\n",
      "             '___PRIOR___': 0.5714285714285714,\n",
      "             'testing': 0.25,\n",
      "             'this': 0.5},\n",
      " 'this': {'___BLANKUNITPROB___': 0.33333333333333337,\n",
      "          '___NUMBLANKGRAMS___': 1,\n",
      "          '___PRIOR___': 0.2857142857142857,\n",
      "          'out': 0.3333333333333333,\n",
      "          'testing': 0.3333333333333333}}\n"
     ]
    }
   ],
   "source": [
    "data        = \"testing this testing this out testing testing\"\n",
    "basic_model = utils.basicLanguageModel(data, gram_size = 1)\n",
    "\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "print('Langugage model structure for data: \"testing this testing this out testing testing\"')\n",
    "print('-----------------------------------------------------------------------------------------------------------')\n",
    "pprint(basic_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> As you can see from the structure of the output above, our `basicLangaugeModel` is similar to the language model we demonstrated in the last tutorial, but has a couple of important modifications:\n",
    "\n",
    "1. More computationally efficient because it doesn't keep track of (given ngram, next ngram) pairs with zero incidence.\n",
    "2. The probabilities of the next ngram, given the current ngram are adjusted for a Laplacian smoothing assumption.\n",
    "3. `___TOTALTOKENS___`: keeps track of the total tokens used in the training data\n",
    "4. `___UNIQUETOKENS___`: keeps track of the number of unique tokens in the training data \n",
    "5. For each given ngrams, we provide:\n",
    "    * `___PRIOR___` : absolute indicidence of the given ngram in the training data \n",
    "    * `___NUMBLANKGRAMS___`: number of `___UNIQUETOKENS___` that did not occur as next ngrams, for a given_gram \n",
    "    * `___BLANKUNITPROB___`: probability of a missing next ngram, assuming it occured only once; this is used for Laplacian smoothing.\n",
    "   \n",
    "We can validate that the language model is providing sensible values by checking that the sum of the `__PRIOR__` probabilities and the conditional probabilities sum to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUM OF NGRAM PRIOR PROBABILITIES: 1.0\n",
      "SUM OF NEXT WORD PROBABILITIES, FOR GIVEN NGRAM \"testing\":  1.0\n",
      "SUM OF NEXT WORD PROBABILITIES, FOR GIVEN NGRAM \"this\":  1.0\n",
      "SUM OF NEXT WORD PROBABILITIES, FOR GIVEN NGRAM \"out\":  1.0\n"
     ]
    }
   ],
   "source": [
    "# Testing that the prior probabilties sum to 1\n",
    "priors = []\n",
    "for given_ngram in basic_model:\n",
    "    if '___' not in given_ngram:\n",
    "        priors.append(basic_model[given_ngram]['___PRIOR___'])\n",
    "print('SUM OF NGRAM PRIOR PROBABILITIES:', sum(priors))\n",
    "\n",
    "# Testing that the conditional probabilities sum to 1\n",
    "\n",
    "#For a given ngram in the model\n",
    "for given_ngram in basic_model:\n",
    "    conditional = []\n",
    "    if '___' not in given_ngram:\n",
    "        for next_token in basic_model[given_ngram]:\n",
    "            if '___' not in next_token:\n",
    "                # Add the probabilities of the known next grams\n",
    "                conditional.append(basic_model[given_ngram][next_token])\n",
    "        \n",
    "        # Add the probabilities of the missing next ngrams\n",
    "        conditional.append(basic_model[given_ngram]['___BLANKUNITPROB___'] * basic_model[given_ngram]['___NUMBLANKGRAMS___'])\n",
    "        print('SUM OF NEXT WORD PROBABILITIES, FOR GIVEN NGRAM \"' + given_ngram + '\": ', sum(conditional))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> One of the things that was discussed in Lecture 2 that we didn't get a chance to practice in the homework was `back-off` -  the strategy of using shorter ngrams when we can't find a match for a larger ngram. Building the components that support `back-off` is pretty straight forward; all we'll need to do is train is retrain our language model for various ngram sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_size    = 3 \n",
    "Russel_lm     = utils.trainBackoffModel(corpora = Russel   , max_gram_size = ngram_size)\n",
    "Nietzsche_lm  = utils.trainBackoffModel(corpora = Nietzsche, max_gram_size = ngram_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> The `trainBackoffModel` function shown above is a simple `for` loop that trains 3 `basicLanguageModels` of varying gram sizes and stores them in a dictionary for later use. For instance, the `Russel_lm` contains our `basicLanguageModel` trained using the Russel corpus with n-grams of size 3, 2, and 1. We can see the n-gram language model conditional probabilities for `Russel` by simply indexing the list of models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional Probability from uigram language model: p(doctrine | this)\n",
      "0.0006556947085437021 \n",
      "\n",
      "Conditional Probability from bigram language model: p(doctrine may | this doctrine)\n",
      "9.206661940580204e-06 \n",
      "\n",
      "Conditional Probability from trigram language model: p(doctrine may be | this doctrine may)\n",
      "4.314436103201312e-06 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Conditional Probability from uigram language model: p(doctrine | this)')\n",
    "print(Russel_lm[0]['this']['doctrine'],'\\n')\n",
    "\n",
    "print('Conditional Probability from bigram language model: p(doctrine may | this doctrine)')\n",
    "print(Russel_lm[1]['this doctrine']['doctrine may'],'\\n')\n",
    "\n",
    "print('Conditional Probability from trigram language model: p(doctrine may be | this doctrine may)')\n",
    "print(Russel_lm[2]['this doctrine may']['doctrine may be'],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Now that we've trained the our `basicLanguageModels`, we can use them to assess the probabilities of new sentences being generated by the `Russel` and `Nietzsche` language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the sentence: \" attain salvation through reason \". The Predicted authors is:  Russel\n",
      "For the sentence: \" attain salvation through power \". The Predicted authors is:  Nietzsche\n",
      "For the sentence: \" ______ salvation through power \". The Predicted authors is:  Nietzsche\n",
      "For the sentence: \" the mighty shall inherit the earth \". The Predicted authors is:  Nietzsche\n",
      "For the sentence: \" contemplation may refine the mind \". The Predicted authors is:  Russel\n",
      "For the sentence: \" contemplation may ______ the mind \". The Predicted authors is:  Russel\n"
     ]
    }
   ],
   "source": [
    "# Test sentences:\n",
    "sentences = ['attain salvation through reason', \n",
    "             'attain salvation through power',\n",
    "             '______ salvation through power',\n",
    "             'the mighty shall inherit the earth', \n",
    "             'contemplation may refine the mind',\n",
    "             'contemplation may ______ the mind',]\n",
    "\n",
    "# Predictions:\n",
    "for sentence in sentences:\n",
    "    r_logprob = utils.evaluateBackoffLangaugeModel(sentence, Russel_lm)\n",
    "    n_logprob = utils.evaluateBackoffLangaugeModel(sentence,Nietzsche_lm)\n",
    "    author = 'Russel' if r_logprob > n_logprob else 'Nietzsche'\n",
    "    print('For the sentence: \"', sentence, '\". The Predicted authors is: ', author)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> Notice here that because of back-off and Laplacian smoothing, our model is robust to missing terms such as the long `______` segments I inserted into the sample sentences. If you have some familiarity with the works of `Russel` and `Neitzsche` these sample sentence results will intuitively make sense and, at the surface, imply that we can directly use our language models to discriminate between the authors!  But in order for us to really understand how well these models can discriminate between the works of Russel and Neitzsche, we'll need to formally test our models on some data that we didn't use when training the models. That's where you come in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 1: \n",
    "#### Worth 1/5 Points\n",
    "\n",
    "#### A. Retrain the Language Models using 80% of the data:\n",
    "As we've discussed in the lectures, it is common practice to use 80% of one's data for model training and to test the performance of our models on the remaining 20% of the data that was held out. For this learning exercise, you will retrain the `Russel` and `Neitzsche` language models (tri-gram, with back-off) from Part 1 of the tutorial using 4/5 of the books from each author. You are welcome to use any code from this tutorial, including any functions in `materials/code/utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Test your Language Models on the remaining 20% of the data:\n",
    "Evaluate the performance of the language models you trained in Part A of the learning exercise on the 2 books in your testing set (i.e. the two books we didn't use to train your model). More specifically, split the the testing set into a list of sentences and use the model to obtain the log probabilities for each sentence according to the `Russel` and `Neitzsche` models. Display these probabilities as four histograms using `matplotlib`. That is, your four plots should show:\n",
    "\n",
    "1. The probabilities of the Russel test data according to the Russel model\n",
    "2. The probabilities of the Russel test data according to the Neitzsche model\n",
    "3. The probabilities of the Neitzsche test data according to the Russel model\n",
    "4. The probabilities of the Neitzsche test data according to the Neitzsche model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Interpretation of results:\n",
    "Plese comment on any notable differences you see between the empirical distributions from part B. Speculate on why those differences might exist and what the results imply about the ability of the language models (in their current form) to classify new unseen sentences as belonging to `Russel` or `Neitzsche`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Part 2: Better Classification using N-gram Language Models\n",
    "In the previous section we used two language models trained on two seperate corpora to classify if new sentences we had never seen before may have come from `Neitzsche` or `Russel`. One deficiency of the our previous approach is that our language models were trained to model the language they observed in their respective training datasets, not to formally discriminate between the authors of the texts! For this reason, it may not be fair to directly compare the probabilities from one model to another.\n",
    "\n",
    "To illustrate, let us consider the following example using the same `BackoffLanguageModel` from the previous part of this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR TEST SENTENCE:  do not care\n",
      "MODEL X: log[p(\"do not care\")] =  -7.783640596221254\n",
      "MODEL Y: log[p(\"do not care\")] =  -9.230731061623919\n"
     ]
    }
   ],
   "source": [
    "ngram_size  = 1 \n",
    "\n",
    "# train model x\n",
    "data_x      = \"I do not care about applications a\"\n",
    "model_x     = utils.trainBackoffModel(data_x   , ngram_size)\n",
    "\n",
    "# train model y\n",
    "data_y      = \"I I I I I I do not care about applications x y z \"\n",
    "model_y     = utils.trainBackoffModel(data_y   , ngram_size)\n",
    "\n",
    "test_sentence = \"do not care\"\n",
    "prob_x = utils.evaluateBackoffLangaugeModel(test_sentence, model_x)\n",
    "prob_y = utils.evaluateBackoffLangaugeModel(test_sentence, model_y)\n",
    "\n",
    "print('FOR TEST SENTENCE: ', test_sentence)\n",
    "print('MODEL X: log[p(\"'+ test_sentence + '\")] = ', prob_x) \n",
    "print('MODEL Y: log[p(\"'+ test_sentence + '\")] = ', prob_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Whoa! What's going on here? Why is there such a difference between the models for the test sentence `do not care` when it shows up once and only once in both training corpora? Let's take a peek at the probabilities for a given uni-gram to see if we can discover what might be happening here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL X:\n",
      "{'___BLANKUNITPROB___': 0.14285714285714288,\n",
      " '___NUMBLANKGRAMS___': 6,\n",
      " '___PRIOR___': 0.14285714285714285,\n",
      " 'not': 0.14285714285714285}\n",
      "\n",
      "MODEL Y:\n",
      "{'___BLANKUNITPROB___': 0.1111111111111111,\n",
      " '___NUMBLANKGRAMS___': 8,\n",
      " '___PRIOR___': 0.07142857142857142,\n",
      " 'not': 0.1111111111111111}\n"
     ]
    }
   ],
   "source": [
    "print('MODEL X:')\n",
    "pprint(model_x[0]['do'])\n",
    "\n",
    "print('\\nMODEL Y:')\n",
    "pprint(model_y[0]['do'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> It seems that the probability of the uni-gram `not` given the uni-gram `do` is different for models X and Y. The reason for this difference is because the vocabulary size (the number of unique uni-grams) for the language models is different and hence, the Laplacian smoothing has a different impact on the conditional probabilities of the uni-grams! That is, the training data for `model_x` had 7 `___UNIQUETOKENS___`, while the training data for `model_y` had 9 `___UNIQUETOKENS___` - more unseen uni-grams means more probability density allocated to the unseen uni-rgams, at the cost of the probability density allocated to the seen uni-grams. Let's assume that we adjust for this by adding in the missing tokens from one data set, into the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Missing from data_x\n",
      "['x', 'z', 'y']\n",
      "\n",
      "Vocabulary Missing from data_y\n",
      "['a']\n"
     ]
    }
   ],
   "source": [
    "ngram_size  = 1 \n",
    "\n",
    "# train model x\n",
    "data_x      = \"I do not care about applications a\"\n",
    "data_y      = \"I I I I I I do not care about applications x y z \"\n",
    "\n",
    "x_grams = utils.extract_word_ngrams(data_x,1)\n",
    "y_grams = utils.extract_word_ngrams(data_y,1)\n",
    "\n",
    "missing_from_x = list(set(x_grams + y_grams) - set(x_grams))\n",
    "missing_from_y = list(set(x_grams + y_grams) - set(y_grams))\n",
    "\n",
    "print('Vocabulary Missing from data_x')\n",
    "print(missing_from_x)\n",
    "\n",
    "print('\\nVocabulary Missing from data_y')\n",
    "print(missing_from_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> Now that we know what's missing, let's append those terms to the end of the data accordingly and re-compare the probability of the next uni-gram being `not` for a given uni-gram `do`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL X:\n",
      "{'___BLANKUNITPROB___': 0.09090909090909091,\n",
      " '___NUMBLANKGRAMS___': 10,\n",
      " '___PRIOR___': 0.07692307692307693,\n",
      " 'not': 0.09090909090909091}\n",
      "\n",
      "MODEL Y:\n",
      "{'___BLANKUNITPROB___': 0.09090909090909091,\n",
      " '___NUMBLANKGRAMS___': 10,\n",
      " '___PRIOR___': 0.0625,\n",
      " 'not': 0.09090909090909091}\n",
      "13\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "data_x  = \"I do not care about applications a \"         + \" ___END___ \" + \" ___END___ \".join(missing_from_x)\n",
    "data_y  = \"I I I I I I do not care about applications x y z \" + \" ___END___ \" + \" ___END___ \".join(missing_from_y)\n",
    "\n",
    "model_x     = utils.trainBackoffModel(data_x   , ngram_size)\n",
    "model_y     = utils.trainBackoffModel(data_y   , ngram_size)\n",
    "\n",
    "test_sentence = \"do not care\"\n",
    "prob_x = utils.evaluateBackoffLangaugeModel(test_sentence, model_x)\n",
    "prob_y = utils.evaluateBackoffLangaugeModel(test_sentence, model_y)\n",
    "\n",
    "print('MODEL X:')\n",
    "pprint(model_x[0]['do'])\n",
    "\n",
    "print('\\nMODEL Y:')\n",
    "pprint(model_y[0]['do'])\n",
    "\n",
    "print(model_x[0]['___TOTALTOKENS___'])\n",
    "print(model_y[0]['___TOTALTOKENS___'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> That's better! But there's still another critical difference here. Notice that the prior probability is lower in `model_y` than it is in `model_x`. This is, once again, because models x and y have a different number of total words in their training data sets (7 in x, 11 in y). If we are ok eliminating this prior information, we can compare the model only according to their conditional probabilities by simply dividing out the prior from the results (i.e. by setting `prior=False` in my `utils.evaluateBackoffLangaugeModel`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR TEST SENTENCE:  do not care\n",
      "MODEL X: log[p(\"do not care\")] =  -7.193685818395112\n",
      "MODEL Y: log[p(\"do not care\")] =  -7.193685818395112\n"
     ]
    }
   ],
   "source": [
    "#importlib.reload(utils)\n",
    "\n",
    "import math\n",
    "\n",
    "data_x  = \"I do not care about applications a \"         + \" ___END___ \" + \" ___END___ \".join(missing_from_x)\n",
    "data_y  = \"I I I I I I do not care about applications x y z \" + \" ___END___ \" + \" ___END___ \".join(missing_from_y)\n",
    "\n",
    "model_x     = utils.trainBackoffModel(data_x   , 1)\n",
    "model_y     = utils.trainBackoffModel(data_y   , 1)\n",
    "\n",
    "test_sentence = \"do not care\"\n",
    "prob_x = utils.evaluateBackoffLangaugeModel(test_sentence, model_x, prior = False)\n",
    "prob_y = utils.evaluateBackoffLangaugeModel(test_sentence, model_y, prior = False) \n",
    "\n",
    "print('FOR TEST SENTENCE: ', test_sentence)\n",
    "print('MODEL X: log[p(\"'+ test_sentence + '\")] = ', prob_x) \n",
    "print('MODEL Y: log[p(\"'+ test_sentence + '\")] = ', prob_y) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 2: \n",
    "#### Worth 1/5 Points\n",
    "\n",
    "#### A. Retrain the Language Models using 80% of the data:\n",
    "Repeat the procedure from Learning Exercise 1 A. after augmenting the vocabulary of the training data sets to include the missing tokens as shown in this section of the tutorial. Note: because our model uses back-off, you will need to include the missing tokens for each of the n-gram sizes within the corresponding training data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Test your Language Models on the remaining 20% of the data:\n",
    "Repeat the procedure from Learning Exercise 1 B. after excluding the prior probabilities of the initial n-gram as shown in this section of the tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Interpret Results:\n",
    "Compare the distributions from Learning Exercise 1 part B. to the distributions from Learning Exercise 2 part B. Comment on any important differences, and if those differences are statistically significant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Comparing Model Perplexity\n",
    "As we discussed briefly in lecture 2, language models are usually assessed through their [perplexity](https://en.wikipedia.org/wiki/Perplexity). Create a function to compute the perplexity of the `basicLanguageModel` and compare the perplexity of the tri-gram `Russel` and `Neitzsche` models you trained in this section of the Learning Exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Part 3: Classification using Classification Models\n",
    "As we've seen together in the previous two sections of this tutorial, language models can be used for classification purposes by comparing the probability that a given sequence of text was generated by one author, versus another. But if our objective is to classify the author of a text, we need not spend so much time building a language model; instead we can focus our attention on the classification task directly. \n",
    "\n",
    "Let's return to the earlier task of sentence classification, starting with breaking our corpora into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in Nietzsche Books:  12581\n",
      "Sentences in Russel Books:     11038\n",
      "------------------------------------------\n",
      "Here is an example sentence from Russel:\n",
      "------------------------------------------\n",
      "other impulses,\n",
      "though they may grow out of the central principle in the individual,\n",
      "may be injurious to the growth of others, and they need to be checked\n",
      "in the interest of others.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "r_sentences = nltk.sent_tokenize(Russel)\n",
    "n_sentences = nltk.sent_tokenize(Nietzsche)\n",
    "\n",
    "print('Sentences in Nietzsche Books: ', len(n_sentences))\n",
    "print('Sentences in Russel Books:    ', len(r_sentences))\n",
    "\n",
    "print('------------------------------------------')\n",
    "print('Here is an example sentence from Russel:')\n",
    "print('------------------------------------------')\n",
    "print(r_sentences[151])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "As we discussed in the lectures, a bag-of-words is a simple way of representing a text document or segment as the count of the n-grams within it. With that in mind, let's convert the example sentence above into a bag-of-words representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': 2,\n",
      " '.': 1,\n",
      " 'and': 1,\n",
      " 'are': 1,\n",
      " 'be': 1,\n",
      " 'been': 1,\n",
      " 'but': 1,\n",
      " 'development': 1,\n",
      " 'from': 1,\n",
      " 'growth': 1,\n",
      " 'have': 1,\n",
      " 'impulses': 1,\n",
      " 'in': 3,\n",
      " 'injurious': 1,\n",
      " 'instinctive': 1,\n",
      " 'least': 1,\n",
      " 'main': 1,\n",
      " 'others': 1,\n",
      " 'result': 1,\n",
      " 'tend': 1,\n",
      " 'the': 2,\n",
      " 'their': 1,\n",
      " 'those': 1,\n",
      " 'thwarted': 1,\n",
      " 'to': 3,\n",
      " 'unimpeded': 1,\n",
      " 'which': 1,\n",
      " 'who': 1}\n"
     ]
    }
   ],
   "source": [
    "unigram     = utils.extract_word_ngrams(r_sentences[152],1)\n",
    "bag_of_word = utils.CountFrequency(unigram)\n",
    "\n",
    "pprint(bag_of_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br> This tells us that the sentence we were just looking at consisted of 2 `,` uni-grams, 1 `.` uni-gram, and so on. Notice that converting a sentence to its bag-of-words representation eliminates information about the order of the words! That is, if I simply provided you this bag of words, you would have no way of (consistently) reconstructing the original sentence that was used to generate it. So why would anyone use a bag-of-words representation? Because it provides a simple way to convert our sentences into numerical vectors, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "vector = list(bag_of_word.values())\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Once we have transformed a string into a numerical vector, we can treat language like we would any other numerical object! That is, having all of our sentences represented as points in a vector space will allow us to do things like train models that can classify the authors of documents! \n",
    "\n",
    "But before we dive into classification using these vectors, we'll need to address a deficiency of the bag of words representation shown in the above example. Consider the fact that any given sentence is likely to only contain a small number of the total words in a given vocabulary. Consequently, if we want to compare two vectors, we'll need to keep track of both the words from the vocabulary that showed up, as well as those that did not! Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the distinct unigrams from the Russel books and the Nietzsche books and combine them\n",
    "vocabulary    = list(set( utils.extract_word_ngrams(Russel, 1) + utils.extract_word_ngrams(Nietzsche, 1) ))\n",
    "\n",
    "# Get the sentence I want to cast as bag of words:\n",
    "unigram = utils.extract_word_ngrams(r_sentences[152],1)\n",
    "\n",
    "# Convert to bag of words:\n",
    "tainted_bow  = utils.CountFrequency(unigram + vocabulary)         # append the vocabulary to make sure it's counted\n",
    "bag_of_words = {k: v - 1 for k, v in tainted_bow.items()}         # remove the counts that came from the vocabulary\n",
    "vector       = list(bag_of_words.values())                        # cast to a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words representation (showing first 50 entries, only):\n",
      "[1, 3, 2, 1, 2, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print('Bag of Words representation (showing first 50 entries, only):')\n",
    "print(vector[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>The above code is not the most computationally efficient way to create the bag of words representation, but it's useful to help you understand exactly what a bag-of-words representation is capturing, and how it is generated. In reality, we would want to store our bag of words representation of the text in a sparse array instead of a memory-inefficient dictionary or list. Fortunately, there are Python packages that take care of creating bag-of-words representations in only a few lines of code. Let's use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) function within Python's [Sklearn Library](https://scikit-learn.org/stable/) to convert the sentences we extracted earlier into their bag-of-words represenations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words representation for 11038 Russel sentences and 12581\n",
      "Yields a array, X of size: 23619 sentences x 22281 tokens\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Generate an array that casts the language to a bag of words representation:\n",
    "vectorizer = CountVectorizer()\n",
    "X          = vectorizer.fit_transform(r_sentences + n_sentences)\n",
    "X          = X.toarray()\n",
    "\n",
    "\n",
    "print('Bag of words representation for', len(r_sentences), 'Russel sentences and', len(n_sentences))\n",
    "print('Yields a array, X of size:',np.size(X,0), 'sentences x', np.size(X,1), 'tokens')\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Note that the `CountVectorizer` object operates on the sentences themselves, performs the tokenization, and can even compute the representation for various n-gram sizes by setting `ngram_range` value of the function.\n",
    "\n",
    "Now that we have a numerical representation of our text, we can start training machine learning algorithms with it. Recall from the lecture that supervised classification methods help our models learn a mathematical transformation of some given data, such as the words in a sentence, into some other data that we would like to make predictions about, such as the author of the sentence. We already created a numerical representation of the text, so all we need now is a numerical representation of the authors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.asarray([1 for i in range(0,len(r_sentences))] + [0 for i in range(0,len(n_sentences))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Learning Exercise 3:\n",
    "### Worth 2/5 Points\n",
    "As we discussed in the lectures, Naive bayes refers to a simple probabilistic classifier based on Bayes' theorem, with some strong independence assumptions about the conditional relationships between features. For this learning exercise you will explore the properties of Naive Bayes, and a few other classification models, given a bag of words representation of the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Train and Assess a Naive Bayes Model\n",
    "The code block below trains a Naive Bayes Model on 80% of the data from the tutorial, and tests the model on the remaining 20%. Please study and extend the code block provided below to:\n",
    "\n",
    "1. Use bi-grams instead of unigrams as the \"words\" in the bag of words model\n",
    "3. Perform 10-fold cross validation instead of an 80% - 20% validation\n",
    "2. Report the mean and standard deviation of the following performance metrics across the ten validation folds:\n",
    "    * accuracy, precision, recall and area under the reciever operator curve \n",
    "\n",
    "**PLEASE NOTE:** You must compute the accuracy, precision, recall, and area under the ROC curve using functions you write yourself, not functions from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives:  7819\n",
      "False Positives: 1057\n",
      "True Negatives:  8669\n",
      "False Negatives: 1351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes     import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics         import confusion_matrix\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# initialize a Nieve bayes model\n",
    "naive      = MultinomialNB()\n",
    "\n",
    "# Fit the model using the training data\n",
    "classifier = naive.fit(X_train,y_train)\n",
    "\n",
    "# predict the author of the held-out test sentences\n",
    "predict    = classifier.predict(X_test)\n",
    "\n",
    "# generate the confusion matrix\n",
    "cm         = confusion_matrix(y_test, predict)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# print the confusion matrix components\n",
    "print('True Positives: ',  tp)\n",
    "print('False Positives:',  fp)\n",
    "print('True Negatives: ',  tn)\n",
    "print('False Negatives:',  fn)\n",
    "\n",
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> COMMENT ON YOUR RESULTS.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Construct a dataset that highlights the limitations of Naive Bayes\n",
    "Construct an example dataset of 10-20 sentences that highlights how the feature independence assumptions of Naive Bayes can lead to poor performance. Train Naive Bayes using the dataset you construct and comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> COMMENT ON YOUR RESULTS</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Train other models and compare performance\n",
    "Plot a [reciever operator curve](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/), and a [calibration plot](https://changhsinlee.com/python-calibration-plot/) for each models listed below trained on a given 80%-20% split of your data.\n",
    "\n",
    "1. The Nieve Bayes classification model from part A.\n",
    "3. Sklearn's implementation of [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)  with `none` as the penalty\n",
    "4. Sklearn's implementation of [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) with `elasticnet` as the penalty\n",
    "\n",
    "Please plot the results in a way that makes it easy to compare the performance of the four models. If one of your models fails to converge due to [co-linearities](http://www.stat.tamu.edu/~hart/652/collinear.pdf), don't worry - simply report that in your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> COMMENT ON ANY NOTABLE DIFFERENCES IN THE PERFORMANCE OF THE METHODS, AND DISCUSS WHY THESE DIFFERENCES MIGHT EXIST</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Explore the impact of vocabulary on model performance:\n",
    "Repeat the procedure in part C after reducing the number of features in your bag of words. More specifically, regenerate the plots after removing:\n",
    "* bottom 1% of ngrams by rank\n",
    "* bottom 10% of ngrams by rank\n",
    "* bottom 25% of ngrams by rank\n",
    "* top 1% of ngrams by rank \n",
    "* top 10% of ngrams by rank \n",
    "* top 25% of ngrams by rank \n",
    "\n",
    "Comment on any differences you see in the results based on these changes and speculate on why the removal does (or does not) impact the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT DISCUSSSION HERE</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Part 4: Sentiment Classification\n",
    "In the previous section of this assignment, we demonstrated how to train a model that classifies the author of a text using a bag-of-words. We were able to accomplish this task with relative ease using a supervised learning approach. \n",
    "\n",
    "The sentence-author classification problem we tacked in the last section was convenient because we could easily generate the labels for the training data sentences by simply referring back to the author of the book. That is, all sentences in a book are the product of the author, by definition. \n",
    "\n",
    "But not all classification problems in NLP are as straight forward as the author classification task we solved in the last section. What if we wanted to classify the sentences in `Russel` and `Nietzsche` according to their sentiment? What would we use for training data? Furthermore, sentiment is not binary; it exists on a spectrum. How would we account for that even if we were to label the sentences?  \n",
    "\n",
    "One solution to this problem is to assign a sentiment value to individual words, and to then compute the sentiment of the text based on the properties of those word-level sentiment scores. \n",
    "\n",
    "Fortunately for us, there are resources that provide normalized estimates of word sentiment! One such resource is [SentiWordNet](https://github.com/aesuli/SentiWordNet). SentiWordNet assigns various words in the English language a `positive`, `negative`, and `objective` value score that is normalized between the range of 0 - 1 . You can download the SentiWordNet data from [this address](https://raw.githubusercontent.com/aesuli/SentiWordNet/master/data/SentiWordNet_3.0.0.txt), but I've also included a copy [locally here](materials/sentiwordnet/sentiwordnet.txt). Let's start by importing the data, formatting it and storing it in a dictionary called `sentiment`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARY\n",
    "import csv\n",
    "\n",
    "# INIT DICT TO STORE WORDS AND THEIR SENTIMENT SCORES\n",
    "sentiment = {}\n",
    "\n",
    "# OPEN FILE\n",
    "with open('materials/sentiwordnet/sentiwordnet.txt', newline = '') as f:                                                                                          \n",
    "\n",
    "    # POINT TO CONTENTS\n",
    "    csvreader = csv.reader(f, delimiter='\\t')\n",
    "    \n",
    "    # LOOP THROUGH EACH LINE\n",
    "    for i, line in enumerate(csvreader):\n",
    "        \n",
    "        # GET HEADERS\n",
    "        if i == 0:\n",
    "            headers = line\n",
    "        \n",
    "        # OTHERWISE PROCESS DATA\n",
    "        else:\n",
    "        \n",
    "            # WORD IS 4th COLUMN\n",
    "            words = line[4]\n",
    "            \n",
    "            for word in words.split():\n",
    "                # SAVE POS AND NEG SCORE OF WORD\n",
    "                sentiment[word] = {'PosScore': line[2], 'NegScore': line[3]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it's imported, let's see what the sentiment value of the word `happy` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PosScore': '0.875', 'NegScore': '0'}\n"
     ]
    }
   ],
   "source": [
    "# FIND SCORE OF HAPPY \n",
    "print(sentiment['happy#1']) #1 MEANS ITS FIRST SENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how about the word `sad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PosScore': '0.125', 'NegScore': '0.75'}\n"
     ]
    }
   ],
   "source": [
    "print(sentiment['sad#1']) #1 MEANS ITS FIRST SENSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because words are assigned both a positive an negative score, I can simplify this sentiment value down to a single number by taking the difference between the positive and negative sentiment values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.625\n"
     ]
    }
   ],
   "source": [
    "print(float(sentiment['sad#1']['PosScore']) - float(sentiment['sad#1']['NegScore'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Exercise 4:\n",
    "### Worth 1/5 Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Extract sentiment:\n",
    "Use the word-level sentiment scores provided by SentiWordNet to assign a sentiment score to every word, in every sentence of the Russel and Nietzsche texts. For each sentence, sum the sentiment of all words in that sentence and divide by the total number of words in that sentence to create a single normalized sentiment value for each sentence. Generate two histogram that compares the empirical distribution of sentence sentiments for the two authors. Comment on any differences between the distributions. Are the differences statistically significant?\n",
    "\n",
    "NOTE: For simplicity, please assume the first `sense` of the word in SentiWordNet, as shown in the tutorial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Use of sentiment as a feature:\n",
    "Train a logistic regression model which takes as input \n",
    "* a bag-of-words representation of the sentences (uni-grams) and \n",
    "* the sentence sentiment you just computed\n",
    "\n",
    "to predict the identity of the author, just as we did in Learning Exercise 3. Compute the [odds ratio](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2938757/) of the coefficient that describes the importance of the sentiment feature, and interpret what the odds ratio means. If your model does not converge, use a coherent strategy to remove bag-of-words features until it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color:red\"> Self Assessment </span></h1>\n",
    "Please provide an assessment of how successfully you accomplished the learning exercises in this assignment according to the instruction provided; do not assign yourself points for effort. This self assessment will be used as a starting point when I grade your assignments. Please note that if you over-estimate your grade on a given learning exercise, you will face a 50% penalty on the total points granted for that exercise. If you underestimate your grade, there will be no penalty.\n",
    "\n",
    "* Learning Exercise 1: \n",
    "    * <span style=\"color:red\">X</span>/1 points\n",
    "* Learning Exercise 2: \n",
    "    * <span style=\"color:red\">X</span>/1 points\n",
    "* Learning Exercise 3:\n",
    "    * <span style=\"color:red\">X</span>/2 points\n",
    "* Learning Exercise 4:\n",
    "    * <span style=\"color:red\">X</span>/1 points\n",
    "\n",
    "#### Total Grade: \n",
    "<span style=\"color:red\">X</span>/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
