{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 1\n",
    "Created by Prof. [Mohammad M. Ghassemi](https://ghassemi.xyz)\n",
    "\n",
    "Submitted by: <span style=\"color:red\"> INSERT YOUR NAME HERE </span>\n",
    "\n",
    "In collaboration with: <span style=\"color:red\"> INSERT YOUR HOMEWORK PARTNER'S GRADE HERE </span>\n",
    "\n",
    "\n",
    "<hr> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Goals\n",
    "The goal of this assignment is to familiarize yourself with:\n",
    "1. Regular expressions, \n",
    "2. Text normalization, \n",
    "3. Edit distance, \n",
    "4. N-gram language models and smoothing \n",
    "\n",
    "This assignment combines tutorial components, with learning exercises that you must complete and submit. The learning exercise sections are clearly demarcated within the assignments.\n",
    "\n",
    "## Assumptions\n",
    "BEFORE YOU START\n",
    "\n",
    "1. PULL THE LATEST VERSION OF THE `course-materials` REPOSITORY, AND COPY `homework/HW1/` INTO THE CORRESPONDING DIRECTORY. \n",
    "2. CREATE AND ATTACHED TO A VIRTUAL ENVIRONMENT, AND INSTALLED THE REQUIREMENTS IN `requirements.txt`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from materials.code import class_utils  \n",
    "#Me: class_utils 只包含CountFrequency這個function\n",
    "\n",
    "def CountFrequency(my_list): \n",
    "  \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    for item in my_list: \n",
    "        if (item in freq): \n",
    "            freq[item] += 1\n",
    "        else: \n",
    "            freq[item] = 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Part 0: Collecting Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can perform any text analysis, we will need to collect some text to apply our analysis to! One excellent resource for free text data is [Project Gutenburg](https://www.gutenberg.org/) (PG); PG contains ~60,000 free eBooks in a variety of formats including the very easy to use `.txt` format. Let's start by pulling [Bertrand Russell's](https://plato.stanford.edu/entries/russell/) book on [The Problems of Philosophy](http://www.gutenberg.org/cache/epub/5827/pg5827.txt) directly from the internet to our machines using [Python's requests library](https://requests.readthedocs.io/en/master/). The requests library is a powerful tool for extracting text data from the internet. Given that most contemporary text data is on the web, it's a tool you should become familiar with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "website = 'http://www.gutenberg.org/cache/epub/5827/pg5827.txt'\n",
    "site    = requests.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Server status code: 200\n",
      "--------------------------------------------------------\n",
      "Here is a 250 character sample of your text:\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "wledge in the world which is so certain that no\r\n",
      "reasonable man could doubt it? This question, which at first sight might\r\n",
      "not seem difficult, is really one of the most difficult that can\r\n",
      "be asked. When we have realized the obstacles in the way of a\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Server status code: ' + str(site.status_code))\n",
    "print('--------------------------------------------------------')\n",
    "print('Here is a 250 character sample of your text:')\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n",
    "print(site.text[1500:1750])\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requests object returned a server status code of `200`, and placed the text we requested in `r.text`. The status code is the server's way of telling us `OK` but note that servers can also refuse to serve us; If we request another resources that doesn't exist from Guttenburg for example, it will respond differently. Let's ask for a new book called `idontthinkyouhavethisbook.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = 'http://www.gutenberg.org/cache/epub/5827/idontthinkyouhavethisbook.txt'\n",
    "r = requests.get(website)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Server status code: 404\n",
      "--------------------------------------------------------\n",
      "Here is a 250 character sample of your text:\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "_logo\"> -->\n",
      "  <a id=\"main_logo\" href=\"/\" class=\"no-hover\">\n",
      "    <img src=\"/gutenberg/pg-logo-129x80.png\" alt=\"Project Gutenberg\" draggable=\"false\" />\n",
      "  </a>\n",
      "  <!--\t</div>-->\n",
      "  <div id=\"menu\">\n",
      "    <label for=\"tm\" id=\"toggle-menu\">Menu<span class=\"drop-\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Server status code: ' + str(r.status_code))\n",
    "print('--------------------------------------------------------')\n",
    "print('Here is a 250 character sample of your text:')\n",
    "print('- - - - - - - - - - - - - - - - - - - - - - - - - - - - ')\n",
    "print(r.text[1500:1750])\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the server returned a `404` status code, which is it's way of telling us `Not Found`. Sure enough, the content that was returned in `r.text` is the error page of Project Guttenburg, not the book we requested. I point this out because when you collect your own data from the internet, especially large volumes of data, you will want to pay attention to those server status codes to make sure what you requested is what you obtained. To learn more about server codes, you can see this [this page](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "# Part 1: Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regular expression is a compact programming language that is specialized for enhanced text search. We can import the `re` library, which comes standard with the Python programming language, to analyze the book we pulled from Project Guttenburg in part 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_expression = 'philosophy'\n",
    "text               = site.text\n",
    "results            = re.search(regular_expression,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "A match for the search query: \"philosophy\"\n",
      "was found at the following character span: (764, 774)\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('A match for the search query: \"' + results.group() + '\"\\nwas found at the following character span: ' + str(results.span()))\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example finds the first instance of the word `philosophy` in the book. For some purposes, we may want to capture *every instance* where the search term was mentioned in a text. We can accomplish that using the `re.finditer` function, and packaging our results in a neat list of dictionaries that contain the matching string and the location of the match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the mentions of the search term\n",
    "regular_expression = 'philosophy'\n",
    "results            = [ {\"indicies\":m.span(), \"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "matches            = []; [matches.append(m['match']) for m in results];\n",
    "#Me: \n",
    "#matches           = len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "We found 72 matches...\n",
      "\n",
      "We found 72 matches...\n",
      "\n",
      "[{'indicies': (764, 774), 'match': 'philosophy'}, {'indicies': (1833, 1843), 'match': 'philosophy'}, {'indicies': (1849, 1859), 'match': 'philosophy'}, {'indicies': (5109, 5119), 'match': 'philosophy'}, {'indicies': (19891, 19901), 'match': 'philosophy'}, {'indicies': (21170, 21180), 'match': 'philosophy'}, {'indicies': (33483, 33493), 'match': 'philosophy'}, {'indicies': (33560, 33570), 'match': 'philosophy'}, {'indicies': (33849, 33859), 'match': 'philosophy'}, {'indicies': (50321, 50331), 'match': 'philosophy'}, {'indicies': (54760, 54770), 'match': 'philosophy'}, {'indicies': (103297, 103307), 'match': 'philosophy'}, {'indicies': (109118, 109128), 'match': 'philosophy'}, {'indicies': (122400, 122410), 'match': 'philosophy'}, {'indicies': (122529, 122539), 'match': 'philosophy'}, {'indicies': (125952, 125962), 'match': 'philosophy'}, {'indicies': (126061, 126071), 'match': 'philosophy'}, {'indicies': (130439, 130449), 'match': 'philosophy'}, {'indicies': (137039, 137049), 'match': 'philosophy'}, {'indicies': (137797, 137807), 'match': 'philosophy'}, {'indicies': (141982, 141992), 'match': 'philosophy'}, {'indicies': (142856, 142866), 'match': 'philosophy'}, {'indicies': (143182, 143192), 'match': 'philosophy'}, {'indicies': (187623, 187633), 'match': 'philosophy'}, {'indicies': (217060, 217070), 'match': 'philosophy'}, {'indicies': (217606, 217616), 'match': 'philosophy'}, {'indicies': (218230, 218240), 'match': 'philosophy'}, {'indicies': (218493, 218503), 'match': 'philosophy'}, {'indicies': (231308, 231318), 'match': 'philosophy'}, {'indicies': (231368, 231378), 'match': 'philosophy'}, {'indicies': (231474, 231484), 'match': 'philosophy'}, {'indicies': (232593, 232603), 'match': 'philosophy'}, {'indicies': (233131, 233141), 'match': 'philosophy'}, {'indicies': (233425, 233435), 'match': 'philosophy'}, {'indicies': (233545, 233555), 'match': 'philosophy'}, {'indicies': (233816, 233826), 'match': 'philosophy'}, {'indicies': (234323, 234333), 'match': 'philosophy'}, {'indicies': (234555, 234565), 'match': 'philosophy'}, {'indicies': (235193, 235203), 'match': 'philosophy'}, {'indicies': (235369, 235379), 'match': 'philosophy'}, {'indicies': (235447, 235457), 'match': 'philosophy'}, {'indicies': (235666, 235676), 'match': 'philosophy'}, {'indicies': (235850, 235860), 'match': 'philosophy'}, {'indicies': (235992, 236002), 'match': 'philosophy'}, {'indicies': (236350, 236360), 'match': 'philosophy'}, {'indicies': (236378, 236388), 'match': 'philosophy'}, {'indicies': (236439, 236449), 'match': 'philosophy'}, {'indicies': (236606, 236616), 'match': 'philosophy'}, {'indicies': (236725, 236735), 'match': 'philosophy'}, {'indicies': (237387, 237397), 'match': 'philosophy'}, {'indicies': (237505, 237515), 'match': 'philosophy'}, {'indicies': (237868, 237878), 'match': 'philosophy'}, {'indicies': (238560, 238570), 'match': 'philosophy'}, {'indicies': (238691, 238701), 'match': 'philosophy'}, {'indicies': (238775, 238785), 'match': 'philosophy'}, {'indicies': (238849, 238859), 'match': 'philosophy'}, {'indicies': (238890, 238900), 'match': 'philosophy'}, {'indicies': (238988, 238998), 'match': 'philosophy'}, {'indicies': (239235, 239245), 'match': 'philosophy'}, {'indicies': (239324, 239334), 'match': 'philosophy'}, {'indicies': (239976, 239986), 'match': 'philosophy'}, {'indicies': (240137, 240147), 'match': 'philosophy'}, {'indicies': (240278, 240288), 'match': 'philosophy'}, {'indicies': (240616, 240626), 'match': 'philosophy'}, {'indicies': (241278, 241288), 'match': 'philosophy'}, {'indicies': (241368, 241378), 'match': 'philosophy'}, {'indicies': (241511, 241521), 'match': 'philosophy'}, {'indicies': (241613, 241623), 'match': 'philosophy'}, {'indicies': (242858, 242868), 'match': 'philosophy'}, {'indicies': (249062, 249072), 'match': 'philosophy'}, {'indicies': (249535, 249545), 'match': 'philosophy'}, {'indicies': (249777, 249787), 'match': 'philosophy'}]\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(matches)) + ' matches...\\n')\n",
    "#Me:\n",
    "#print('We found ' +  str(len(results)) + ' matches...\\n')\n",
    "print(str(results))\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found `72` matches for the search term. That seems a bit low for a book with the word philosophy in it's name! Let's update our regular expression so that it's not sensitive to capitalization of the letter `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the mentions of the search term\n",
    "regular_expression = '(p|P)hilosophy'\n",
    "results            = [ {\"indicies\":m.span(), \"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "matches            = []; [matches.append(m['match']) for m in results];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "We found 81 matches...\n",
      "\n",
      "['Philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'Philosophy']\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(results)) + ' matches...\\n')\n",
    "print(str(matches))\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found `81` matches; that is not much better than before. Let's make the search even more general by seeking out terms that start with the word `philo`, followed by an arbitrary number of [a-z] characters thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_expression = '(P|p)hilo[a-z]+'\n",
    "results            = [ {\"indicies\":m.span(), \"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "matches            = []; [matches.append(m['match']) for m in results];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "We found 154 matches...\n",
      "\n",
      "['Philosophy', 'Philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosopher', 'philosopher', 'philosopher', 'Philonous', 'Philonous', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'Philosophy', 'philosophy', 'philosophy', 'philosopher', 'philosophical', 'Philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophical', 'philosopher', 'philosophical', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosophy', 'philosophical', 'philosopher', 'philosophy', 'philosopher', 'philosophy', 'philosopher', 'philosophy', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'philosopher', 'philosophers', 'philosophy', 'philosophy', 'philosophical', 'philosophers', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophers', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophers', 'philosophies', 'philosophers', 'philosophical', 'philosopher', 'philosophers', 'philosophers', 'philosophers', 'philosophy', 'philosophical', 'philosophy', 'philosophers', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophers', 'philosophers', 'philosophers', 'Philosophical', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophical', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'Philosophy', 'philosophy', 'philosopher', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophy', 'philosophers', 'philosophy', 'philosophical', 'philosophy', 'philosophy', 'philosophy', 'philosophy', 'philosophize', 'Philosophy', 'philosophy', 'philosophic', 'philosophic', 'Philosophic', 'Philosophic', 'philosophic', 'philosophies', 'philosophical', 'philosophic', 'philosophic', 'philosophic', 'philosophy', 'Philosophy', 'philosophy', 'philosophy', 'philosophers', 'Philonous', 'Philosophy']\n",
      "--------------------------------------------------------\n",
      "We found 11 distinct terms...\n",
      "\n",
      "{'Philosophy': 9, 'philosophy': 72, 'philosophers': 37, 'philosopher': 11, 'Philonous': 3, 'philosophical': 10, 'philosophies': 2, 'Philosophical': 1, 'philosophize': 1, 'philosophic': 6, 'Philosophic': 2}\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(matches)) + ' matches...\\n')\n",
    "print(str(matches))\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(set(matches))) + ' distinct terms...\\n')\n",
    "#print(class_utils.CountFrequency(matches))\n",
    "print(CountFrequency(matches))\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks better! But now we have a new problem. It seems that an inconveniently named philosopher, `Philonous`, has made his way into our results. Let's modify our regular expression to use a `negative lookahead` `(?!nous)` to remove him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_expression = '(P|p)hilo(?!nous)[a-z]+'\n",
    "results            = [ {\"indicies\":m.span(), \"match\":m.group() } for m in re.finditer(regular_expression,text)];\n",
    "matches            = []; [matches.append(m['match']) for m in results];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "We found 0 matches...\n",
      "\n",
      "[]\n",
      "--------------------------------------------------------\n",
      "We found 0 distinct terms...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'class_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-0344fe33d48b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--------------------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'We found '\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' distinct terms...\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCountFrequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--------------------------------------------------------'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'class_utils' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(matches)) + ' matches...\\n')\n",
    "print(str(matches))\n",
    "print('--------------------------------------------------------')\n",
    "print('We found ' +  str(len(set(matches))) + ' distinct terms...\\n')\n",
    "print(class_utils.CountFrequency(matches))\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 1: \n",
    "#### Worth 1/5 Points\n",
    "Now I'm curious how Bertrand used these various philosophical terms in his sentences, and how Bertrand's language, more generally, compares to [Friedrich Nietzsche](https://en.wikipedia.org/wiki/Friedrich_Nietzsche) and other philosophers. You're going to help me with that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Download Data\n",
    "Use Python's `requests` library to collect Friedrich Nietzsche's book, [Beyond Good and Evil](http://www.gutenberg.org/cache/epub/4363/pg4363.txt) into Python. Collect one additional work of philosophy (of your choice) and import it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################\n",
    "import re\n",
    "import requests\n",
    "#download the book <Beyond Good and Evil> by Friedrich Nietzsche\n",
    "website_FN = 'https://www.gutenberg.org/cache/epub/4363/pg4363.txt'\n",
    "site_FN    = requests.get(website_FN)\n",
    "site_FN.encoding = site_FN.apparent_encoding\n",
    "text_FN    = site_FN.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Basic Text Cleansing\n",
    "Use `re.sub` to do some basic text cleansing on books you downloaded. More specifically: \n",
    "\n",
    "1. replace the various forms of whitespace that appear (`\\n`,`\\r`,`\\t` etc.) with a single whitespace character. \n",
    "2. cast all text to lower case\n",
    "3. replace common abbreviations with their full form, for instance: `it's` with `it is`\n",
    "\n",
    "Note that item #3 does not have to be comprehensive, just do a couple common abbreviations to illustrate that you know how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################\n",
    "#1. replace the various forms of whitespace that appear (\\n,\\r,\\t etc.) with a single whitespace character.\n",
    "regular_expression = '(\\n|\\r|\\t|\\s)+'  \n",
    "text_FN = re.sub(regular_expression, \" \", text_FN)\n",
    "\n",
    "#2. cast all text to lower case\n",
    "text_FN = text_FN.lower()\n",
    "\n",
    "#3. replace common abbreviations with their full form, for instance: it's with it is\n",
    "#Ref: https://www.kaggle.com/life2short/data-processing-replace-abbreviation-of-word\n",
    "abbr_dict = {\n",
    "    \"what's\":\"what is\",\n",
    "    \"what're\":\"what are\",\n",
    "    \"who's\":\"who is\",\n",
    "    \"who're\":\"who are\",\n",
    "    \"where's\":\"where is\",\n",
    "    \"where're\":\"where are\",\n",
    "    \"when's\":\"when is\",\n",
    "    \"when're\":\"when are\",\n",
    "    \"how's\":\"how is\",\n",
    "    \"how're\":\"how are\",\n",
    "\n",
    "    \"i'm\":\"i am\",\n",
    "    \"we're\":\"we are\",\n",
    "    \"you're\":\"you are\",\n",
    "    \"they're\":\"they are\",\n",
    "    \"it's\":\"it is\",\n",
    "    \"he's\":\"he is\",\n",
    "    \"she's\":\"she is\",\n",
    "    \"that's\":\"that is\",\n",
    "    \"there's\":\"there is\",\n",
    "    \"there're\":\"there are\",\n",
    "\n",
    "    \"i've\":\"i have\",\n",
    "    \"we've\":\"we have\",\n",
    "    \"you've\":\"you have\",\n",
    "    \"they've\":\"they have\",\n",
    "    \"who've\":\"who have\",\n",
    "    \"would've\":\"would have\",\n",
    "    \"not've\":\"not have\",\n",
    "\n",
    "    \"i'll\":\"i will\",\n",
    "    \"we'll\":\"we will\",\n",
    "    \"you'll\":\"you will\",\n",
    "    \"he'll\":\"he will\",\n",
    "    \"she'll\":\"she will\",\n",
    "    \"it'll\":\"it will\",\n",
    "    \"they'll\":\"they will\",\n",
    "\n",
    "    \"isn't\":\"is not\",\n",
    "    \"wasn't\":\"was not\",\n",
    "    \"aren't\":\"are not\",\n",
    "    \"weren't\":\"were not\",\n",
    "    \"can't\":\"can not\",\n",
    "    \"couldn't\":\"could not\",\n",
    "    \"don't\":\"do not\",\n",
    "    \"didn't\":\"did not\",\n",
    "    \"shouldn't\":\"should not\",\n",
    "    \"wouldn't\":\"would not\",\n",
    "    \"doesn't\":\"does not\",\n",
    "    \"haven't\":\"have not\",\n",
    "    \"hasn't\":\"has not\",\n",
    "    \"hadn't\":\"had not\",\n",
    "    \"won't\":\"will not\",\n",
    "    #'[\"\\'?,\\.]':'', #replace all these punctuation with whitespace\n",
    "    '\\'':'',         #remove '\\s' E.g. plato\\'s  --> plato's\n",
    "    '\\s+':' ', # replace multi space with one single space  \n",
    "}\n",
    " \n",
    "for abbreviation, text in abbr_dict.items():\n",
    "    text_FN = re.sub(abbreviation, text, text_FN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Sentence Extractor\n",
    "Write a regular expression that will extract the complete sentences containing any matches of the following regex we wrote together earlier: `(P|p)hilo(?!nous)[a-z]+`. Apply this regular expression to \"Beyond Good and Evil\", \"The Problems of Philosophy\", and the work of philosophy you chose. Show a couple of example sentences from each work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR RESULTS TO THE SCREEN.\n",
    "################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "regular_expression = '[^\\s].+?(?!\\.)(beyond good and evil).+?(\\.|\\?|:|\\!|;|\\*)\\s'  \n",
    "#[^\\s].+?(beyond good and evil).+?(\\.|\\?|:|\\!|\\*)\\s   [14種]能偵測句尾，但無法偵測句首\n",
    "#句尾: .?:!;*\n",
    "\n",
    "results            = [ {\"indicies\":m.span(), \"match\":m.group() } for m in re.finditer(regular_expression,text_FN)];\n",
    "matches            = []; [matches.append(m['match']) for m in results];\n",
    "\n",
    "len(results)\n",
    "#寫到這，１4種"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#寫到這\n",
    "#temp = matches[11]\n",
    "results  = [ {\"indicies\":m.span(), \"match\":m.group() } for m in re.finditer('(;|\\.|\\?)\\s(?!\\.)beyond good and evil', temp)];\n",
    "matches  = []; [matches.append(m['match']) for m in results];\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C. Sentence Extractor\n",
    "\n",
    "#pip install pysbd\n",
    "\n",
    "#code: https://pythonrepo.com/repo/nipunsadvilkar-pySBD-python-natural-language-processing\n",
    "#paper:https://aclanthology.org/2020.nlposs-1.15.pdf\n",
    "import pysbd\n",
    "seg = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "text_sentences = seg.segment(text_FN)\n",
    "\n",
    "#use <matches> to store sentences containing 'beyond good and evil'\n",
    "matches = []; \n",
    "\n",
    "for m in text_sentences:  \n",
    "    if(re.findall('beyond good and evil', m) != []):\n",
    "        matches.append(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. Sentence Tokenizer\n",
    "Write another regular expression that tokenizes the text in your collected sentences by splitting the sentences into a list of individual words. Tokenize the entire text of all three books using this regular expression. For each book, store your result as a list of lists and print the last word of the last sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR RESULTS TO THE SCREEN.\n",
    "################################################################################\n",
    "#D. Sentence Tokenizer\n",
    "\n",
    "#Tokenizer Method 1 (no punctuations)\n",
    "words = [];\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "for m in matches: \n",
    "    words = words + tokenizer.tokenize(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer Method 2 (keep punctuations)\n",
    "#words = [];\n",
    "\n",
    "#rx = \"\\w+(?:'\\w+)?|[^\\w\\s]\"\n",
    "#for m in matches: \n",
    "#    words = words + re.findall(rx, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E. Compare Works\n",
    "Use Python's `matplotlib` library, to compare the distribution of the most common words that show up in each book. Compare the distributions and comment on any important statistical similarities or differences between the distributions. Do the distributions of the word frequency follow Zipf's law? Ignoring common words (`the`,`and`,`in`, etc.) what are the top 10 words that each book used most frequently? Are there any words that showed up more frequently in one work than they did in another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI8AAAD8CAYAAAD+DIR+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHN1JREFUeJzt3Xm4JXdZJ/Dvm4QtLAnYjTJA08BAFBjWBlE2DYtAFERZBQREe0YRRIeBoLKIj0xUBFRciBiJEHEIBAQCkhBCIjudEMgGwgMtBJhJI2oAkRjyzh+nrlzaW31Pd9+zdPfn8zznuVV16tTvved365y63/OrOtXdAQAAAIC1HLboAgAAAABYXsIjAAAAAEYJjwAAAAAYJTwCAAAAYJTwCAAAAIBRwiMAAAAARgmPAAAAABglPAIAAABglPAIAAAAgFFHLLqAaWzatKm3bt266DIAAAAADhrnnXfel7t783rrHRDh0datW7Njx45FlwEAAABw0Kiqf5hmPaetAQAAADBKeAQAAADAKOERAAAAAKOERwAAAACMEh4BAAAAMGpm4VFVnVRVl1fVRbstf3pVfbKqLq6q35lV+wAAAADsv1mOPHp1kgevXlBVP5zk4Unu2N23T/KSGbYPAAAAwH6aWXjU3ecm+cpui38+yQnd/c1hnctn1T4AAAAA+2/e1zy6bZL7VNWHquqcqrr7nNsHAAAAYC8csYD2bpjknknunuT1VXWr7u7dV6yq7Um2J8mWLVvmWuQsbT3+9EWXcFDYecJxiy4BAAAADgnzHnl0WZLTeuLDSa5OsmmtFbv7xO7e1t3bNm/ePNciAQAAAJiYd3j05iTHJklV3TbJNZN8ec41AAAAADClmZ22VlWvS/JDSTZV1WVJXpDkpCQnVdVFSa5M8qS1TlkDAAAAYDnMLDzq7seN3PWEWbUJAAAAwMaa92lrAAAAABxAhEcAAAAAjBIeAQAAADBKeAQAAADAKOERAAAAAKOERwAAAACMEh4BAAAAMEp4BAAAAMAo4REAAAAAo4RHAAAAAIwSHgEAAAAwSngEAAAAwCjhEQAAAACjhEcAAAAAjBIeAQAAADBKeAQAAADAKOERAAAAAKNmFh5V1UlVdXlVXbTGfc+qqq6qTbNqHwAAAID9N8uRR69O8uDdF1bVzZM8MMnnZtg2AAAAABtgZuFRd5+b5Ctr3PWyJM9O0rNqGwAAAICNMddrHlXVw5J8obs/Ns92AQAAANg3R8yroao6MsmvJXnQlOtvT7I9SbZs2TLDymBi6/GnL7qEg8bOE47b8G3qn40zi/4BAAAOXvMceXTrJLdM8rGq2pnkZknOr6rvWWvl7j6xu7d197bNmzfPsUwAAAAAVsxt5FF3X5jkxivzQ4C0rbu/PK8aAAAAANg7Mxt5VFWvS/KBJMdU1WVV9dRZtQUAAADAbMxs5FF3P26d+7fOqm0AAAAANsZcv20NAAAAgAOL8AgAAACAUcIjAAAAAEYJjwAAAAAYJTwCAAAAYJTwCAAAAIBRwiMAAAAARgmPAAAAABglPAIAAABglPAIAAAAgFHCIwAAAABGCY8AAAAAGCU8AgAAAGCU8AgAAACAUcIjAAAAAEYJjwAAAAAYJTwCAAAAYNTMwqOqOqmqLq+qi1Yt+92q+kRVfbyq3lRVR8+qfQAAAAD23yxHHr06yYN3W3Zmkjt09x2T/H2S586wfQAAAAD208zCo+4+N8lXdlt2RndfNcx+MMnNZtU+AAAAAPtvkdc8+pkk7xi7s6q2V9WOqtqxa9euOZYFAAAAwIqFhEdV9WtJrkpyytg63X1id2/r7m2bN2+eX3EAAAAA/Icj5t1gVT0pyY8muX9397zbBwAAAGB6cw2PqurBSZ6T5H7d/a/zbBsAAACAvTez09aq6nVJPpDkmKq6rKqemuQVSa6f5MyquqCq/nRW7QMAAACw/2Y28qi7H7fG4j+fVXsAAAAAbLxFftsaAAAAAEtOeAQAAADAKOERAAAAAKOERwAAAACMEh4BAAAAMEp4BAAAAMAo4REAAAAAo4RHAAAAAIwSHgEAAAAwSngEAAAAwCjhEQAAAACjhEcAAAAAjBIeAQAAADBKeAQAAADAKOERAAAAAKOERwAAAACMEh4BAAAAMGqq8Kiq7rC3G66qk6rq8qq6aNWyG1XVmVX1qeHnDfd2uwAAAADMz7Qjj/60qj5cVb9QVUdP+ZhXJ3nwbsuOT3JWd98myVnDPAAAAABLaqrwqLvvneTxSW6eZEdV/VVVPXCdx5yb5Cu7LX54kpOH6ZOT/PjelQsAAADAPE19zaPu/lSSX0/ynCT3S/IHVfWJqvqJvWjvu7v7S8P2vpTkxmMrVtX2qtpRVTt27dq1F00AAAAAsFGmvebRHavqZUkuTXJskh/r7u8bpl82i8K6+8Tu3tbd2zZv3jyLJgAAAABYx7Qjj16R5Pwkd+rup3X3+UnS3V/MZDTStP5fVd0kSYafl+9NsQAAAADM17Th0UOT/FV3fyNJquqwqjoySbr7NXvR3luSPGmYflKSv9mLxwIAAAAwZ9OGR+9Kcp1V80cOy0ZV1euSfCDJMVV1WVU9NckJSR5YVZ9K8sBhHgAAAIAldcSU6127u7+2MtPdX1sZeTSmux83ctf9py0OAAAAgMWaduTR16vqriszVXW3JN+YTUkAAAAALItpRx49M8mpVfXFYf4mSR4zm5IAAAAAWBZThUfd/ZGq+t4kxySpJJ/o7n+faWUAAAAALNy0I4+S5O5Jtg6PuUtVpbv/ciZVAQAAALAUpgqPquo1SW6d5IIk3xoWdxLhEQAAAMBBbNqRR9uS3K67e5bFAAAAALBcpv22tYuSfM8sCwEAAABg+Uw78mhTkkuq6sNJvrmysLsfNpOqADhgbD3+9EWXcNDYecJxiy4BAAD+k2nDoxfOsggAAAAAltNU4VF3n1NVt0hym+5+V1UdmeTw2ZYGAAAAwKJNdc2jqvq5JG9I8sph0U2TvHlWRQEAAACwHKa9YPbTktwryRVJ0t2fSnLjWRUFAAAAwHKYNjz6ZndfuTJTVUck6dmUBAAAAMCymDY8OqeqfjXJdarqgUlOTfLW2ZUFAAAAwDKYNjw6PsmuJBcm+e9J3p7k12dVFAAAAADLYdpvW7s6yZ8NNwAAAAAOEVOFR1X12axxjaPuvtWGVwQAAADA0pgqPEqybdX0tZM8KsmN9rXRqvrlJD+bSSB1YZKndPe/7ev2AAAAAJiNqa551N3/uOr2he5+eZJj96XBqrppkmck2dbdd0hyeJLH7su2AAAAAJitaU9bu+uq2cMyGYl0/f1s9zpV9e9Jjkzyxf3YFgAAAAAzMu1pa7+3avqqJDuTPHpfGuzuL1TVS5J8Lsk3kpzR3Wfsvl5VbU+yPUm2bNmyL00BAAAAsJ+m/ba1H96oBqvqhkkenuSWSf45yalV9YTufu1ubZ6Y5MQk2bZt23+6WDcAAAAAszftaWu/sqf7u/ule9HmA5J8trt3Dds+LckPJnntHh8FAAAAwNztzbet3T3JW4b5H0tybpLP70Obn0tyz6o6MpPT1u6fZMc+bAcAAACAGZs2PNqU5K7d/dUkqaoXJjm1u392bxvs7g9V1RuSnJ/J9ZM+muH0NAAAAACWy7Th0ZYkV66avzLJ1n1ttLtfkOQF+/p4AAAAAOZj2vDoNUk+XFVvStJJHpHkL2dWFQAAAABLYdpvW/utqnpHkvsMi57S3R+dXVkAAAAALIPD9mLdI5Nc0d2/n+SyqrrljGoCAAAAYElMFR5V1QuSPCfJc4dF10jy2lkVBQAAAMBymHbk0SOSPCzJ15Oku7+Y5PqzKgoAAACA5TBteHRld3cmF8tOVV13diUBAAAAsCymDY9eX1WvTHJ0Vf1ckncl+bPZlQUAAADAMpj229ZeUlUPTHJFkmOSPL+7z5xpZQDAftt6/OmLLuGgsfOE4zZ0e/pm42x03yT6ZyPNon8AmK91w6OqOjzJO7v7AUkERgAAAACHkHVPW+vubyX516o6ag71AAAAALBEpjptLcm/Jbmwqs7M8I1rSdLdz5hJVQAAAAAshWnDo9OHGwAAAACHkD2GR1W1pbs/190nz6sgAAAAAJbHetc8evPKRFW9cca1AAAAALBk1guPatX0rWZZCAAAAADLZ73wqEemAQAAADgErHfB7DtV1RWZjEC6zjCdYb67+wb70mhVHZ3kVUnukEko9TPd/YF92RYAAAAAs7PH8Ki7D59Ru7+f5G+7+5FVdc0kR86oHQAAAAD2w3ojjzZcVd0gyX2TPDlJuvvKJFfOuw4AAAAA1rfeNY9m4VZJdiX5i6r6aFW9qqquu4A6AAAAAFjH3EceDW3eNcnTu/tDVfX7SY5P8rzVK1XV9iTbk2TLli1zLxIAAFg+W48/fdElHDR2nnDchm9T/2ycje4ffbNxZrHvLLtFjDy6LMll3f2hYf4NmYRJ36G7T+zubd29bfPmzXMtEAAAAICJuYdH3f1/k3y+qo4ZFt0/ySXzrgMAAACA9S3itLUkeXqSU4ZvWvtMkqcsqA4AAAAA9mAh4VF3X5Bk2yLaBgAAAGB6i7jmEQAAAAAHCOERAAAAAKOERwAAAACMEh4BAAAAMEp4BAAAAMAo4REAAAAAo4RHAAAAAIwSHgEAAAAwSngEAAAAwCjhEQAAAACjhEcAAAAAjBIeAQAAADBKeAQAAADAKOERAAAAAKOERwAAAACMEh4BAAAAMEp4BAAAAMCohYVHVXV4VX20qt62qBoAAAAA2LNFjjz6pSSXLrB9AAAAANaxkPCoqm6W5Lgkr1pE+wAAAABMZ1Ejj16e5NlJrl5Q+wAAAABMYe7hUVX9aJLLu/u8ddbbXlU7qmrHrl275lQdAAAAAKstYuTRvZI8rKp2JvnrJMdW1Wt3X6m7T+zubd29bfPmzfOuEQAAAIAsIDzq7ud29826e2uSxyZ5d3c/Yd51AAAAALC+RX7bGgAAAABL7ohFNt7d70nynkXWAAAAAMA4I48AAAAAGCU8AgAAAGCU8AgAAACAUcIjAAAAAEYJjwAAAAAYJTwCAAAAYJTwCAAAAIBRwiMAAAAARgmPAAAAABglPAIAAABglPAIAAAAgFHCIwAAAABGCY8AAAAAGCU8AgAAAGCU8AgAAACAUcIjAAAAAEYJjwAAAAAYNffwqKpuXlVnV9WlVXVxVf3SvGsAAAAAYDpHLKDNq5L8z+4+v6qun+S8qjqzuy9ZQC0AAAAA7MHcRx5195e6+/xh+qtJLk1y03nXAQAAAMD6FnrNo6ramuQuST60yDoAAAAAWNvCwqOqul6SNyZ5Zndfscb926tqR1Xt2LVr1/wLBAAAAGAx4VFVXSOT4OiU7j5trXW6+8Tu3tbd2zZv3jzfAgEAAABIsphvW6skf57k0u5+6bzbBwAAAGB6ixh5dK8kT0xybFVdMNweuoA6AAAAAFjHEfNusLvfm6Tm3S4AAAAAe2+h37YGAAAAwHITHgEAAAAwSngEAAAAwCjhEQAAAACjhEcAAAAAjBIeAQAAADBKeAQAAADAKOERAAAAAKOERwAAAACMEh4BAAAAMEp4BAAAAMAo4REAAAAAo4RHAAAAAIwSHgEAAAAwSngEAAAAwCjhEQAAAACjhEcAAAAAjFpIeFRVD66qT1bVp6vq+EXUAAAAAMD65h4eVdXhSf4oyUOS3C7J46rqdvOuAwAAAID1LWLk0T2SfLq7P9PdVyb56yQPX0AdAAAAAKxjEeHRTZN8ftX8ZcMyAAAAAJZMdfd8G6x6VJIf6e6fHeafmOQe3f303dbbnmT7MHtMkk/OtdBD26YkX150EYzSP8tL3yw3/bO89M1y0z/LS98sN/2zvPTNctM/83WL7t683kpHzKOS3VyW5Oar5m+W5Iu7r9TdJyY5cV5F8W1VtaO7ty26Dtamf5aXvllu+md56Zvlpn+Wl75Zbvpneemb5aZ/ltMiTlv7SJLbVNUtq+qaSR6b5C0LqAMAAACAdcx95FF3X1VVv5jknUkOT3JSd1887zoAAAAAWN8iTltLd789ydsX0TZTcbrgctM/y0vfLDf9s7z0zXLTP8tL3yw3/bO89M1y0z9LaO4XzAYAAADgwLGIax4BAAAAcIAQHrFPqmprVV206DoOJMvynFXVzqratOg6DgVV9f7h51L0/cGiql5dVY/coG09s6qO3Ihtsb6xfaGqXlRVD1hETXAgqqo7V9VDF13HoWIjnu+q+tpG1cN8DO9ZP7XoOmBZCI8AZqS7f3DRNbCuZybZq/Coqg6fUS2HrO5+fne/a9F1wCJU1b5cg/TOSYRH87PQ53sf/0bYf1uTCI9gIDw6RFTV86rqE1V1ZlW9rqqeNXyK8sGq+nhVvamqbjisO7b8blX1sar6QJKnLfQXOnAdUVUnD8/tG6rqyOF5Paeqzquqd1bVTarq1lV1/sqDquo2VXXeMH3/qvpoVV1YVSdV1bWG5Tur6jeq6vzhvu8dln9XVZ0xPOaVSWohv/lBoqqeUFUfrqoLquqVVfW0qvqdVfc/uar+cJj2KeMGqKqfHvaZj1XVa4bF962q91fVZ1ZGIVXV9arqrFX7wMOH5VuH17/d971nJPkvSc6uqrOHdR9UVR8YtnFqVV1vWL6zqp5fVe9N8qj5PwsHlcOr6s+q6uLhtek6q0eTVdUJVXXJ0FcvWXSxh4Kq+pWqumi4PXPYZy7dvZ+GdW9dVX87vGf93cp7DeNGjsHeU1UvrqpzkvxSVW2uqjdW1UeG272Gx95jeK376PDzmKq6ZpIXJXnM8F70mKp64XBM8J7hdfEZq9rf/X3r8OH26qHPL6yqXx7Wfcaq/e+vF/KEzcke+mXbcP+m4bV/ref7usPz/ZGhb1beb55cVacN+8inVh8fDPf/3vD+clZVbR6WrblPDf3z0uH96beHv5Ezh8e/sqr+oYwk/w6r3u9fNfxtn1JVD6iq9w39cY+qulFVvXn4G/9gVd1xeOz9hv69YOjT6yc5Icl9hmW/vNjf7uAwZR/9p9e94bF73L+Yg+52O8hvSbYluSDJdZJcP8mnkjwryceT3G9Y50VJXj5MT7P8d5NctOjf7UC6ZfLpRSe51zB/UpL/leT9STYPyx6T5KRh+uwkdx6mX5zk6UmuneTzSW47LP/LJM8cpncmefow/QtJXjVM/0GS5w/Txw01bFr083Eg3pJ8X5K3JrnGMP/HSZ6U5NOr1nlHknsP019b1ff2l317zm+f5JMrf7NJbpTk1UlOzeQDkNutPP+ZfIPoDYbpTUk+nUlYuta+96xheueqbW9Kcm6S6w7zz1m17+xM8uxFPx8H+m3oi6tWvba9PskThj595NC/n8y3v9Dj6EXXfLDfktwtyYVJrpvkekkuTnKXtfppmD4ryW2G6e9P8u5F/w7LfMv4Mdh7kvzxqvX+atV7x5Yklw7TN0hyxDD9gCRvHKafnOQVqx7/wkyOJ641vJb9Y5JrjLxv/fTQ72euevzRw88vJrnW6mUH422dftk2rLMpyc6R5/vFq/aJo5P8/bAPPTnJZ5Iclckx2z8kufmwXid5/DD9/JXtje1Tw+vi25IcPsy/Islzh+kHx/HcWv26dXjt+m+ZHCOcl8l7fiV5eJI3J/nDJC8Y1j82yQXD9Fvz7eOE62VyTPFDSd626N/rYLpN2Ud7et1bc/9ym8/NEMhDw72T/E13fyNJquqtmbzBHd3d5wzrnJzk1Ko6asrlr0nykLn9BgePz3f3+4bp1yb51SR3SHJmVSXJ4Um+NNz/qiRPqapfySRUukeSY5J8trv/fljn5ExGgb18mD9t+Hlekp8Ypu+7Mt3dp1fVP83g9zpU3D+TA+6PDP11nSSXJ/lMVd0zk4PPY5K8b3QL7K1jk7yhu7+cJN39leG5f3N3X53kkqr67mHdSvLiqrpvkquT3DTJyn2773vPSLL7qJZ7ZhJGvW9o45pJPrDq/v+zkb/YIeyz3X3BMH1eJgeSK65I8m9JXlVVp2fyjxOzde8kb+rurydJVZ2W5D5Zo59qMhLvBzM5Llh5/LXmXO+BZq1jsBWrX1MekOR2q57XGwwjH45KcnJV3SaTsOAae2jr9O7+ZpJvVtXlmbz+jb1vvTXJrWoyUvb0JGcM2/h4klOq6s2Z/BN3sNpTv0zjQUkeVlXPGuavnUnolyRndfe/DNu9JMktMvng7+p8u89fm+S0KfapU7v7W6tqfkSSdPffOp4b9dnuvjBJquriTPqjq+rCTN5vbpHkJ5Oku99dkxH6R2Vy7PbSqjolyWndfdmqPmFjrddHe3rdG9u/mAPh0aFhI175KpOdl/2z+3P41SQXd/cPrLHuG5O8IMm7k5zX3f9YVTdfZ/vfHH5+K9+5f+u7jVFJTu7u537HwqqnJnl0kk9k8k+Y53vjjL32fHO3dZLk8Uk2J7lbd/97Ve3M5IA+a2xjrW1WJp/EP26klq9PVTHrWd1338rkn9kkSXdfVVX3yOQf3scm+cVMAkRmZ+wYYa1+OizJP3f3nWde1cFjT8dgq19TDkvyAythxn88eBLunN3dj6iqrZmMjBmze58dkZH3rWHbd0ryI5l8CPXoJD+TyQjl+yZ5WJLnVdXtu/uqPbR5oBrrl6vy7ct6XHtknZXH/2R3f/I7FlZ9f9buh7V01t+nVv+NSDKms/r5v3rV/NWZ9MVaf8/d3ScMH1o8NMkHy5c4zNJ6ffSbGX/dm3b/YgZc8+jQ8N4kP1ZV1x4+4Tgukzejf6qq+wzrPDHJOUOSu9byf07yL1V172H54+dY/8FkS1WtBEWPS/LBJJtXllXVNarq9knS3f+W5J1J/iTJXwyP+UQmn/7+12H+iUlWRoONOTdDf1XVQ5LccIN+l0PRWUkeWVU3TpLhvPlbZDLi68cz6VOjUzbWWUkeXVXflUye8z2se1SSy4fg6Icz+TRqxe773nuH6a9mcspCMtkf77Wyf9Xkuki33aDfgykM71FHdffbM7mYuZBi9s5N8uPD3/t1MxnZ8HdrrdjdVyT5bFU9Kklq4k7zK/WAtNYx2FrOyCQsTTK5/uQweVSSLwzTT161/urXrj1Z832rJtfKOay735jkeUnuWlWHZXIKyNlJnp3J6VjXm6KNA9FYv+zMZKRWMjmVdsXuz/c7kzy9hqEpVXWXKdo8bNU2fyrJe/dyn3pvJiFfqupBcTy3r1YfF/9Qki939xVVdevuvrC7fzvJjiTfm+n3MzbW2OseCyY8OgR090eSvCXJxzL5J3dHkn/J5Fotv1tVH8/kAP1Fw0PGlj8lyR/V5ILZ3/HJGFO7NMmThuf2Rpmcd/3ITC6E+LFMzr9f/Q1dp2TyydQZyX8ESk/JZHjzhZkk9H+6Tpu/kcnFhc/PZJj15zbu1zm0dPclSX49yRlDH56Z5Cbd/U9JLklyi+7+8CJrPNh098VJfivJOcM+8tI9rH5Kkm1VtSOTA8NPrLpv933vT4blJyZ5R1Wd3d27MjlIed2w3gczOXhkfq6f5G3D839OEhconbHuPj+Ta6t8OMmHMjllek+nwzw+yVOH/fHiTK5RwYg9HIPt7hmZvH59fDgV438My38nyf+uqvdlcmr7irMzOc3tgqp6zB7aX/N9K5PTet9TVRdk0v/PHbb/2uH44qNJXjZ8eHjQ2UO/vCTJz1fV+zO55tGK3Z/v38zkVJqPV9VFw/x6vp7k9jX5ApRj8+3j62n3qd9I8qDheO4hmVzm4KtT/sp82wsz7GuZXBD7ScPyZ9bkAs4fy+T/nHdkchrnVTX5wg7vR/Mz9rrHgq1ckJKDXFVdr7u/VlVHZpK4bx8OGFliw7n0R3X38xZdCxyohiHPb+vuOyy4FOAQ5BhsOR1o/VKTb9f91nB67w8k+ROnkALz5BzBQ8eJVXW7TM7fPnmZ3xyZqKo3Jbl1XO8DAA5kjsGW04HWL1uSvH44vfDKJD+34HqAQ4yRRwAAAACMcs0jAAAAAEYJjwAAAAAYJTwCAAAAYJTwCAAAAIBRwiMAAAAARgmPAAAAABj1/wER0d9NIzPKWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################\n",
    "#E. Compare Works\n",
    "\n",
    "#count frequency of words\n",
    "words_freq = CountFrequency(words)\n",
    "\n",
    "#define common words in words_freq\n",
    "common_words = ['the', 'and', 'of', 'to', 'as', 'a', 'in', 'be', 'it', 'is', 'by', 'this', 'which', \n",
    "                'will', 'with', 'that', 'has', 'was', 'for', 'at', 'into', 'who', 'have', 'or', \n",
    "                'they', 'are', 'had', 'how', 'would', 'could', 'its', 'may', 'these', 'their']\n",
    "\n",
    "#delete frequency counts of these common words\n",
    "for m in common_words: \n",
    "    try: \n",
    "        del words_freq[ｍ] \n",
    "    except KeyError:  #prevent common words not in words_freq\n",
    "        pass\n",
    "\n",
    "#sort frequency by descending order \n",
    "words_freq = {k: v for k, v in sorted(words_freq.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "#get the most 10 frequency words and their frequency\n",
    "top_10_freq        = list(words_freq.values())[:10]\n",
    "most_10_freq_words = list(words_freq.keys())[:10] \n",
    " \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.ylabel('Frequency')\n",
    "plt.bar(most_10_freq_words, height=top_10_freq)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "# Part 2: Text normalization \n",
    "Text normalization is an essential preliminary step to any natural language processing pipeline. To begin, let's re-import `The Problems of Philosophy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "website  = 'http://www.gutenberg.org/cache/epub/5827/pg5827.txt'\n",
    "text     = requests.get(website).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "As you saw in `Learning Exercise 1`, tokenization of text can be performed through the use of Regular Expressions. Familiarity with Regular Expressions is an important part of any NLP apprentice's toolkit but fortunately, it's not the only (or even the best) tool that exists for text normalization tasks! Indeed, for basic text normalization and cleansing, Python has a number of functions that operate on strings directly - no libraries required! \n",
    "\n",
    "Let's begin this section of the tutorial by taking a look at some of these functions, starting with the `replace` function to remove the whitespace characters in our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace \\r\\n with whitespace, then replace \\n with whitespace, then replace \\r with whitespace\n",
    "clean_text = text.replace('\\r\\n',' ').replace('\\n',' ').replace('\\r',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "Results\n",
      "--------------------------------------------------------\n",
      "Before:\n",
      "\r\n",
      "\r\n",
      "I have derived valuable assistance from unpublished writings of G. E.\r\n",
      "Moore and J. M. Keynes: from the former, as regards ...\n",
      "--------------------------------------------------------\n",
      "After:\n",
      "I have derived valuable assistance from unpublished writings of G. E. Moore and J. M. Keynes: from the former, as regards ...\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\n')\n",
    "print('--------------------------------------------------------')\n",
    "print('Results')\n",
    "print('--------------------------------------------------------')\n",
    "print('Before:\\n' + text[1101:1227] + ' ...')\n",
    "print('--------------------------------------------------------')\n",
    "print('After:\\n' + clean_text[1056:1177] + ' ...')\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems to have worked! Notice that `replace` is a method that is built into Python strings. That's the reason that we write `text.replace(...)` and not something like `replace(text,...)`.  \n",
    "\n",
    "Python has other many [other useful string manipulation methods](https://www.programiz.com/python-programming/methods/string), but for the rest of the basic pre-processing we'll be used in this tutorial, we'll only need `split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sentences when we see a '.' character\n",
    "clean_text = clean_text.split('.')\n",
    "\n",
    "# Split each sentence further - when there are no arguments provided to split, it defaults to whitespace (tabs, newlines, spaces, etc)\n",
    "textmat = []\n",
    "for c in clean_text:\n",
    "    textmat.append(c.split())    \n",
    "#re.split(r'\\w+|\\$[\\d\\.]+|\\S+',\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Results (showing sentence 10)\n",
      "--------------------------------------------------------\n",
      "['I', 'have', 'also', 'profited', 'greatly', 'by', 'the', 'criticisms', 'and', 'suggestions', 'of', 'Professor', 'Gilbert', 'Murray']\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the tenth sentence:\n",
    "print('--------------------------------------------------------')\n",
    "print('Results (showing sentence 10)')\n",
    "print('--------------------------------------------------------')\n",
    "print(textmat[10])\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing shown above is mostly for illustrative purposes. If we'd like to do anything serious, we'll need a more sophisticated approach to sentence segmentation and text tokenization. Fortunately, there are many existing tools in Python that assist with the Sentence segmentation and word tokenization. One of the most popular tools for this is the [Natural Langauge Toolkit](https://www.nltk.org/). Let's import the tool into python and explore some of it's functionality together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Li-\n",
      "[nltk_data]     Wen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the NLTK Package\n",
    "#to install nltk on Google Cloud\n",
    "#conda install nltk\n",
    "#pip install nltk\n",
    "import nltk\n",
    "\n",
    "# Importing the `punkt` data, used by the NLTK tokenizer\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test out the NLTK tokenizer with a tricky sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "['At', 'eight', \"o'clock\", 'on', 'Thursday', 'morning', 'Arthur', 'did', \"n't\", 'feel', 'very', 'good', '.', 'He', 'drove', 'to', 'the', 'store', 'in', 'New', 'York', 'at', '100', 'm.p.h.', '!', 'He', 'wanted', 'to', 'obtain', 'some', 'Kool-Aid', '!', 'It', 'cost', '$', '10.89', '.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good. He drove to the store in New York at 100 m.p.h.! He wanted to obtain some Kool-Aid! It cost $10.89.\"\"\"\n",
    "tokens    = nltk.word_tokenize(sentences)\n",
    "print('\\n')\n",
    "print(tokens)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tokenization is pretty good for something out-of-the box, but it could be better! In an ideal world, we would want the Tokenizer to understand that `New York` is actually a single token, not two. Furthermore, depending on our use case, we may want the `$` sign to be attached to the `10.89` token. So, if you wanted to use the NLTK tool out-of-the-box, you may need to do some additional pre-processing. Let's try the sentence splitting functionality next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[\"At eight o'clock on Thursday morning Arthur didn't feel very good.\", 'He drove to the store in New York at 100 m.p.h.!', 'He wanted to obtain some Kool-Aid!', 'It cost $10.89.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = nltk.sent_tokenize(sentences)\n",
    "print('\\n' + str(result) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one also works rather nicely. Indeed, NLTK does a lot to make Natural Language Processing accessible. In fact, NLTK has several other tools that we will cover in subsequent lectures, but one of the tools that is not yet available in NLTK (at the time this tutorial was written) is Byte Pair Encoding, the tokenization approaches with some relation to many of today's NLP powerhouses (Yes, I'm talking about [BERT](https://en.wikipedia.org/wiki/BERT_(language_model))). I think I'm going to need some help with that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 2:\n",
    "#### Worth 1/5 Points\n",
    "\n",
    "For this learning exercise, you will implement the Byte Pair Encoding (BPE) algorithm and use it to Tokenize `The Problems of Philosophy`. If you can't recall how the BPE algorithm works, you can refer back to the lectures, or see Section 2.4.3 of [Daniel Jurafsky & James H. Martin's NLP book](https://web.stanford.edu/~jurafsky/slp3/2.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Download Books to Create a Larger Corpus\n",
    "Given that the BPE algorithm is data-driven, we'll first need to gather a training data corpus. For your training data, you will choose at least 10 books from [Project Gutenburg](https://www.gutenberg.org/): \n",
    "* Choose at least 5 books that were authored by Bertrand Russell, not including `The Problems of Philosophy`\n",
    "* Choose at least 5 books that were authored by Friedrich Nietzsche\n",
    "\n",
    "You'll be using these training data later with BPE to identify the token `vocabularies`, which you will then use to tokenize `The problems of Philosophy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################\n",
    "\n",
    "#5 books that were authored by Bertrand Russell and 5 books that were authored by Friedrich Nietzsche\n",
    "Books = [#authored by Bertrand Russell\n",
    "         'https://www.gutenberg.org/cache/epub/2529/pg2529.txt',\n",
    "         'https://www.gutenberg.org/files/38280/38280-0.txt',\n",
    "         'https://www.gutenberg.org/cache/epub/13940/pg13940.txt',\n",
    "         'https://www.gutenberg.org/files/25447/25447-0.txt',\n",
    "         'https://www.gutenberg.org/files/44932/44932-0.txt'   \n",
    "         #authored by Friedrich Nietzsche\n",
    "        'https://www.gutenberg.org/files/52319/52319-0.txt', \n",
    "        'https://www.gutenberg.org/files/51356/51356-0.txt',    \n",
    "        'https://www.gutenberg.org/files/7205/7205-0.txt',  \n",
    "        'https://www.gutenberg.org/files/52263/52263-0.txt',  \n",
    "        'https://www.gutenberg.org/files/52190/52190-0.txt'\n",
    "        ]  \n",
    "\n",
    "#download all books\n",
    "text_all = \"\"\n",
    "\n",
    "for m in Books:\n",
    "    site = requests.get(m)\n",
    "    site.encoding = site.apparent_encoding\n",
    "    text = site.text\n",
    "    text_all = text_all + \" \" + text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Implement Byte Pair Encoding\n",
    "Implement the Byte Pair Encoding algorithm by filling out the function skeleton below. The function should take three inputs:\n",
    "\n",
    "* `text` : an unprocessed text file that we wish to tokenize using BPE\n",
    "* `k`    : the number of merges to perform\n",
    "* `training_corpus` : a python list of complete texts we wish to use for training. The formatting should look like: `['blah blah blah ...', 'the rain in spain ...', 'do pickles count as vegetables? ...']`\n",
    "\n",
    "The function should output a tokenized version of the `text` based on the `k` vocabulary units learned by BPE using the `training_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BytePairEncoding(text, k, training_corpus):\n",
    "    tokenized_text = []\n",
    "    vocabulary = []\n",
    "    # INSERT YOUR CODE HERE\n",
    "    return vocabulary, tokenized_text\n",
    "\n",
    "#寫到這"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get counts of pairs of consecutive symbols\n",
    "def get_pair_stats(vocab):\n",
    "    pairs = {}\n",
    "    for word, frequency in vocab.items():\n",
    "        symbols = word.split() #list(word)\n",
    "\n",
    "        # count occurrences of pairs\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            current_frequency = pairs.get(pair, 0)\n",
    "            pairs[pair] = current_frequency + frequency\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all occurrences of the most frequent pair\n",
    "def merge_vocab(best_pair, vocab_in):\n",
    "    vocab_out = {}\n",
    "\n",
    "    # re.escape\n",
    "    # ensures the characters of our input pair will be handled as is and\n",
    "    # not get mistreated as special characters in the regular expression.\n",
    "    pattern = re.escape(' '.join(best_pair))\n",
    "    replacement = ''.join(best_pair)\n",
    "\n",
    "    for word_in in vocab_in:\n",
    "        # replace most frequent pair in all vocabulary\n",
    "        word_out = re.sub(pattern, replacement, word_in)\n",
    "        vocab_out[word_out] = vocab_in[word_in]\n",
    "\n",
    "    return vocab_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################   HW Answer  ###################\n",
    "#ref: http://ethen8181.github.io/machine-learning/deep_learning/subword/bpe.html\n",
    "\n",
    "#data cleaning \n",
    "#Replace \\r\\n with whitespace, then replace \\n with whitespace, then replace \\r with whitespace\n",
    "clean_text = text_all.replace('\\r\\n',' ').replace('\\n',' ').replace('\\r',' ')    \n",
    "\n",
    "#word tokenization to count vocabulary frequency (no punctuations)\n",
    "import nltk\n",
    "vocabulary = [];\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "for m in matches: \n",
    "    vocabulary = vocabulary + tokenizer.tokenize(m)\n",
    "    \n",
    "#pre-process vocabulary for BytePair Encoding\n",
    "# <the> --> <t h e </w>>, </w> indicates end of the word\n",
    "vocabulary_processed = []\n",
    "\n",
    "for m in vocabulary: \n",
    "    temp = ' '.join(list(m)) + ' ' + ('</w>')\n",
    "    vocabulary_processed.append(temp)\n",
    "\n",
    "#count frequency    \n",
    "vocab = CountFrequency(vocabulary_processed )    \n",
    " \n",
    "# we store the best pair during each iteration for encoding new vocabulary, more on this later\n",
    "bpe_codes = {}\n",
    "num_merges = 10  # hyperparameter\n",
    "for i in range(num_merges):\n",
    "    print('\\niteration', i)\n",
    "    pair_stats = get_pair_stats(vocab)\n",
    "    if not pair_stats:\n",
    "        break\n",
    "\n",
    "    best_pair = max(pair_stats, key=pair_stats.get)\n",
    "    bpe_codes[best_pair] = i\n",
    "\n",
    "    print('vocabulary: ', vocab)\n",
    "    print('best pair:', best_pair)\n",
    "    vocab = merge_vocab(best_pair, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Evaluate Byte Pair Encoding\n",
    "Because BPE is data-driven, we might expect there to be some differences in the tokenization based on the training corpus, as well as your choice of `k`! To evaluate BPE, you will now experiment with how the tokenization performs for various settings of `k`, and when using different training . More specifically, you will compare the `vocabulary` and `tokenized_text` of Bertrand Russell's `The Problem's of Philosophy` for:\n",
    "\n",
    "* values of `k` ranging from 1,000 to 10,000 in steps of 1,000 and\n",
    "    *  `training_corpus` sets that include: (1) only Bertrand Russell books, (2) only Friedrich Nietzsche books, and (3) all books from groups (1) and (2).\n",
    "\n",
    "For the above conditions, please generate one or more plots using Python's `matplotlib` that compare:\n",
    "* The total number of unique tokens in the tokenized text\n",
    "* The median size of tokens in the tokenized text\n",
    "* The % overlap in the number of distinct unique tokens generated by BPE and NLTK's word tokenizer.\n",
    "\n",
    "Compare the plots, and note any interesting observations about the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "# Part 3: Edit Distance\n",
    "#### Character Level Distance\n",
    "As we discussed in the lectures, Edit Distance is a way to measure the similarity between different sequences of text. Our goal in this section will be to compare and contrast some of the different measures of edit distance that exist so that you'll have a better intuition for how to use them, depending on the problem you're trying to solve. To start, let's import the [`textdistance` library](https://github.com/life4/textdistance), written by [Gram Voronov](https://orsinium.dev/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install textdistance\n",
    "\n",
    "import textdistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `textdistance` library contains python implementations of about 30 edit distance algorithms. Let's begin by trying out the edit distance tool we discussed in class, the [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "levenshtein distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 0\n",
      "\"test\" and \"texts\" is: 2\n",
      "\"test\" and \"   test\" is: 3\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def GetDistance(string1, string2, method):\n",
    "    # Compute the distance and print the results\n",
    "    distance = eval(\"textdistance.\" + method + \"(string1, string2)\")\n",
    "    print('\"' + string1 + '\" and \"' + string2 + '\" is: ' + str(distance))  \n",
    "    \n",
    "print('\\n')\n",
    "print('levenshtein' + \" distance:\")\n",
    "print('--------------------------------------------------------')\n",
    "GetDistance('test','test'      ,'levenshtein')\n",
    "GetDistance('test','texts'     ,'levenshtein')\n",
    "GetDistance('test','   test'   ,'levenshtein')\n",
    "print('--------------------------------------------------------')\n",
    "print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture that the Levenshtein distance counts the total number of `insertion`, `deletion`, and `substitution` actions that a required to transform one string into another and the cost of each edit is always 1. So, as shown in the above example, moving from `\"test\"` to `\"test\"` requires no edits, and therefore has a distance of 0, moving from `\"test\"` to `\"texts\"` requires 1 substitution and one addition and therefore has a distance of 2, moving from `\"test\"` to `\"   test\"`   has a distance of 3 becasue we must remove the  three whitespace characters to the left of the word `\"   test\"` to create a match.\n",
    "\n",
    "The above example is intentionally meant to highlight that the assumptions of the edit distance measures can have consequence for their estimates; Indeed, depending on the problem we are trying to solve, we may want `\"   test\"` and `\"test\"` to be closer than the distance between `\"test\"` and `\"texts\"`. On that note, let's explore the performance of three other edit distance measures: the [Hamming Distance](https://en.wikipedia.org/wiki/Hamming_distance), [Needleman Wunsch](https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm), and [Smith Waterman](https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamming distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 0\n",
      "\"test\" and \"texts\" is: 2\n",
      "\"test\" and \"   test\" is: 6\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "levenshtein distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 0\n",
      "\"test\" and \"texts\" is: 2\n",
      "\"test\" and \"   test\" is: 3\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "needleman_wunsch distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 4.0\n",
      "\"test\" and \"texts\" is: 2.0\n",
      "\"test\" and \"   test\" is: 1.0\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n",
      "smith_waterman distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 4\n",
      "\"test\" and \"texts\" is: 2.0\n",
      "\"test\" and \"   test\" is: 4.0\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distance_measures = ['hamming',  'levenshtein', 'needleman_wunsch', 'smith_waterman']\n",
    "\n",
    "for method in distance_measures:\n",
    "    print(method + \" distance:\")\n",
    "    print('--------------------------------------------------------')\n",
    "    GetDistance('test'      ,'test'      ,method)\n",
    "    GetDistance('test'      ,'texts'     ,method)\n",
    "    GetDistance('test'      ,'   test'   ,method)\n",
    "    print('--------------------------------------------------------')\n",
    "    print('\\n')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through these results together. First, we notice that the Hamming distance between `\"test\"` and `\"   test\"` is 6, which is even higher than what we observed for the Levenshtein distance! To understand why, we can refer to the definition of the [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance): \"The Hamming distance between two strings is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other.\" Given that there are so many white space characters, and that Hamming is only concerned with substitutions, it makes sense that the hamming distance would be higher than the levenshtein distance!\n",
    "\n",
    "At first glance, the results of the [Needleman Wunch](https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm) and [Smith Waterman](https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm) methods seem suprising. Why is the distance between \"test\" and \"test\" 4? Shouldn't it be 0? Once again, the answer here resides in understanding the definition of the \"distance\" metric. Both of these algorithms are actually providing an **alignment score** between the two sequences. \n",
    "\n",
    "Of course, we may also notice that the scores between the two alignment methods are quite different; the reason for this difference has to do with what these alignment scores are exactly measuring: the Smith–Waterman algorithm is designed to find the *segments* in two sequences that have maximal similarities while the Needleman–Wunsch algorithm is designed for alignment between two complete sequences. Let's explore these two methods to make this distinction a little more clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needleman_wunsch distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 4.0\n",
      "\"test\" and \" test\" is: 3.0\n",
      "\"test\" and \"  test\" is: 2.0\n",
      "\"test\" and \"   test\" is: 1.0\n",
      "\"test\" and \"    test\" is: 0.0\n",
      "\"test\" and \"     test\" is: -1.0\n",
      "\"test\" and \"      test\" is: -2.0\n",
      "\"test\" and \"       test\" is: -3.0\n",
      "\n",
      "\n",
      "smith_waterman distance:\n",
      "--------------------------------------------------------\n",
      "\"test\" and \"test\" is: 4\n",
      "\"test\" and \" test\" is: 4.0\n",
      "\"test\" and \"  test\" is: 4.0\n",
      "\"test\" and \"   test\" is: 4.0\n",
      "\"test\" and \"    test\" is: 4.0\n",
      "\"test\" and \"     test\" is: 4.0\n",
      "\"test\" and \"      test\" is: 4.0\n",
      "\"test\" and \"       test\" is: 4.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distance_measures = ['needleman_wunsch', 'smith_waterman']\n",
    "for method in distance_measures:\n",
    "    print(method + \" distance:\")\n",
    "    print('--------------------------------------------------------')\n",
    "    GetDistance('test','test',method)\n",
    "    GetDistance('test',' test',method)\n",
    "    GetDistance('test','  test',method)\n",
    "    GetDistance('test','   test',method)\n",
    "    GetDistance('test','    test',method)\n",
    "    GetDistance('test','     test',method)\n",
    "    GetDistance('test','      test',method)\n",
    "    GetDistance('test','       test',method)\n",
    "    print('\\n')\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the Needleman Wunsh method grows more negative as the absolute difference between the two strings increases. In this sense, the Needleman-Wunsch algorithm is similar to the Levenshtein distance algorithm; the difference being that Levenshtein uses a static penalty cost to any mismatched letters while the Needleman-Wunsch algorithm gives weights to matches and mismatches differently. The Smith Waterman method distance remains 4 because the substring `test` overlaps in both strings regardless of the whitespace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token-Level distance\n",
    "All of our edit distance work up until now has focused on character-level distances and alignment but depending on the problem we want to solve, we may care more about distance at the token level. Fortunately, there are several distance metrics that measure similarity between strings at the token level. One such measure is the [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index), which is the size of the intersection of the token sets divided by the size of the union of the sample sets. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard distance:\n",
      "--------------------------------------------------------\n",
      "\"The fish was delish, and it made quite a dish\" and \"The fish on the dish was made quite delicious!\" is: 0.7843137254901961\n",
      "\"The fish was delish, and it made quite a dish\" and \"Joe eats crabs, crabs do not taste good.\" is: 0.39344262295081966\n",
      "--------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distance_measures = ['jaccard']\n",
    "text1 = \"The fish was delish, and it made quite a dish\"\n",
    "text2 = \"The fish on the dish was made quite delicious!\"\n",
    "text3 = \"Joe eats crabs, crabs do not taste good.\"\n",
    "for method in distance_measures:\n",
    "    print(method + \" distance:\")\n",
    "    print('--------------------------------------------------------')\n",
    "    GetDistance(text1      ,text2      ,method)\n",
    "    GetDistance(text1      ,text3      ,method)\n",
    "    print('--------------------------------------------------------')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be covering all the edit distance functions of the NLP community in great detail, but hopefully this is enough to whet your appetite, and allow you to dig into some of the methods on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 3:\n",
    "#### Worth 1/5 Points\n",
    "#### A. Towards a More Nuanced Edit Distance Cost Metric\n",
    "As we've eluded to here and in the lectures, one deficiency of the Levenshtein cost metric is the assumption that substitutions, edit, and deletion costs are all equal. Your goal for this learning exercise is to modify the cost structure of the `LevenshteinDistanceDP` method shown below so that is assigns 1/2 of the normal penalty for any edits that involve whitespace characters (FYI, the implementation shown below was originally written by [Ahmed Gad](https://blog.paperspace.com/author/ahmed/), [here](https://blog.paperspace.com/implementing-levenshtein-distance-word-autocomplete-autocorrect/#:~:text=The%20Levenshtein%20distance%20is%20a,representing%20the%20distance%20between%20them.&text=In%20this%20tutorial%20the%20Levenshtein,using%20the%20dynamic%20programming%20approach.)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#following Fig 2.17 in textbook (cost of any whitespace is 0.5)\n",
    "def levenshteinDistanceDP(token1, token2):\n",
    "    distances = np.zeros((len(token1) + 1, len(token2) + 1))\n",
    "    \n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        #deletion cost\n",
    "        distances[t1][0] = distances[t1-1][0] + (0.5 if token1[t1-1] == \" \" else 1)\n",
    "    \n",
    "    for t2 in range(1, len(token2) + 1):\n",
    "        #insertion cost\n",
    "        distances[0][t2] = distances[0][t2-1] + (0.5 if token2[t2-1] == \" \" else 1)\n",
    "\n",
    "    for t1 in range(1, len(token1) + 1):\n",
    "        for t2 in range(1, len(token2) + 1):\n",
    "            a = distances[t1][t2 - 1]\n",
    "            b = distances[t1 - 1][t2]\n",
    "            c = distances[t1 - 1][t2 - 1]\n",
    "            \n",
    "            if (token1[t1-1] == token2[t2-1]):\n",
    "                distances[t1][t2] = min(a, b, c)\n",
    "            else:\n",
    "                #insertion cost\n",
    "                a = a + 1 - 0.5*(' ' in token1[t1-1] + token2[t2-1])\n",
    "                #deletion cost\n",
    "                b = b + 1 - 0.5*(' ' in token1[t1-1] + token2[t2-1]) \n",
    "                #replacement cost\n",
    "                c = c + 1 - 0.5*(' ' in token1[t1-1] + token2[t2-1]) \n",
    "                \n",
    "                distances[t1][t2] = min(a, b, c)  \n",
    "\n",
    "    return distances[len(token1)][len(token2)]\n",
    "\n",
    "print(levenshteinDistanceDP(' test', 'test'))\n",
    "print(levenshteinDistanceDP('test ', 'test'))\n",
    "print(levenshteinDistanceDP('t est', 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "# Part 4: N-Gram language Models\n",
    "In this section, we'll be covering n-gram language models which, as the name implies, are language models that use n-grams!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "An n-gram is a contiguous sequence of n items from a given sample of text or speech. Let's begin this section by defining a function that generates n-grams from our text data using the `nltk` library and using it to extract n-grams for n ranging from 1-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word-Level:\n",
      "1-gram:  ['a', 'fish', 'keeps', 'for', 'a']\n",
      "2-gram:  ['a fish', 'fish keeps', 'keeps for', 'for a', 'a day']\n",
      "3-gram:  ['a fish keeps', 'fish keeps for', 'keeps for a', 'for a day', 'a day ;']\n",
      "4-gram:  ['a fish keeps for', 'fish keeps for a', 'keeps for a day', 'for a day ;', 'a day ; a']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# an ngram extractor \n",
    "def extract_word_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# some example text\n",
    "data = \"\"\"a fish keeps for a day; a fish keeps well if you put it in a cold place. \n",
    "          a fish keeps best if you put it in the fridge. If you’ll be having fish, \n",
    "          have it with an apple because one apple a day keeps the doctor away! \n",
    "          You know they also say that a day keeps coming.\"\"\"\n",
    "\n",
    "# printing the first 5 tokens after gramification\n",
    "print(\"\\nWord-Level:\")\n",
    "print(\"1-gram: \", extract_word_ngrams(data, 1)[0:5])\n",
    "print(\"2-gram: \", extract_word_ngrams(data, 2)[0:5])\n",
    "print(\"3-gram: \", extract_word_ngrams(data, 3)[0:5])\n",
    "print(\"4-gram: \", extract_word_ngrams(data, 4)[0:5])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the n-gram extractor I use above leverages the `nltk` tokenizer that we discussed earlier in this assignment. Also, that sample data I showed above should look familiar... As we discussed in the lectures, n-grams can be generated at whatever level of resolution we need for the task at hand; we also don't need any fancy libraries to generate them! If we were interested in extracting n-grams at the character level for instance, we could do that in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Character-Level:\n",
      "1-gram:  ['a', ' ', 'f', 'i', 's', 'h', ' ', 'k', 'e', 'e']\n",
      "2-gram:  ['a ', ' f', 'fi', 'is', 'sh', 'h ', ' k', 'ke', 'ee']\n",
      "3-gram:  ['a f', ' fi', 'fis', 'ish', 'sh ', 'h k', ' ke', 'kee']\n",
      "4-gram:  ['a fi', ' fis', 'fish', 'ish ', 'sh k', 'h ke', ' kee']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# some example text\n",
    "data = \"a fish keeps\"\n",
    "# for a day; a fish keeps well if you put it in a cold place. \n",
    "#          a fish keeps best if you put it in the fridge. If you’ll be having fish, \n",
    "#          have it with an apple because one apple a day keeps the doctor away! \n",
    "#          You know they also say that a day keeps coming.\"\"\"\n",
    "\n",
    "def extract_char_ngrams(data, num):\n",
    "    return [data[i:i+num] for i in range(len(data))][:(-num-1)] \n",
    "\n",
    "# printing the tokens after gramification\n",
    "print(\"\\nCharacter-Level:\")\n",
    "print(\"1-gram: \", extract_char_ngrams(data, 1))  \n",
    "print(\"2-gram: \", extract_char_ngrams(data, 2))  \n",
    "print(\"3-gram: \", extract_char_ngrams(data, 3))  \n",
    "print(\"4-gram: \", extract_char_ngrams(data, 4)) \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Character-Level:\n",
      "1-gram:  ['a', ' ', 'f', 'i', 's', 'h', ' ', 'k', 'e', 'e', 'p', 's']\n",
      "2-gram:  ['a ', ' f', 'fi', 'is', 'sh', 'h ', ' k', 'ke', 'ee', 'ep', 'ps']\n",
      "3-gram:  ['a f', ' fi', 'fis', 'ish', 'sh ', 'h k', ' ke', 'kee', 'eep', 'eps']\n",
      "4-gram:  ['a fi', ' fis', 'fish', 'ish ', 'sh k', 'h ke', ' kee', 'keep', 'eeps']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#My modification:\n",
    "\n",
    "# some example text\n",
    "data = \"\"\"a fish keeps for a day; a fish keeps well if you put it in a cold place. \n",
    "          a fish keeps best if you put it in the fridge. If you’ll be having fish, \n",
    "          have it with an apple because one apple a day keeps the doctor away! \n",
    "          You know they also say that a day keeps coming.\"\"\"\n",
    "\n",
    "def extract_char_ngrams(data, num):\n",
    "    if num == 1:\n",
    "        extract = list(data)\n",
    "    else: \n",
    "        extract = [data[i:i+num] for i in range(len(data))][:(-num+1)]\n",
    "    \n",
    "    return extract \n",
    "\n",
    "# printing the tokens after gramification\n",
    "print(\"\\nCharacter-Level:\")\n",
    "print(\"1-gram: \", extract_char_ngrams(data, 1))  \n",
    "print(\"2-gram: \", extract_char_ngrams(data, 2))  \n",
    "print(\"3-gram: \", extract_char_ngrams(data, 3))  \n",
    "print(\"4-gram: \", extract_char_ngrams(data, 4)) \n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-Grams\n",
    "While n-grams bring together contiguous sequences of words, skip-grams include grams that skip over certain terms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'fish'),\n",
       " ('a', 'keeps'),\n",
       " ('a', 'for'),\n",
       " ('fish', 'keeps'),\n",
       " ('fish', 'for')]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some example text\n",
    "data = \"\"\"a fish keeps for a day; a fish keeps well if you put it in a cold place. \n",
    "          a fish keeps best if you put it in the fridge. If you’ll be having fish, \n",
    "          have it with an apple because one apple a day keeps the doctor away! \n",
    "          You know they also say that a day keeps coming.\"\"\"\n",
    "\n",
    "# Creating skip-grams using the NLTK library\n",
    "skip_gram = list(nltk.skipgrams(nltk.word_tokenize(data),n=2,k=2))\n",
    "skip_gram[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Models \n",
    "A language model refers to any method that assign probabilities to strings. Let's try to develop a more concrete understanding of language models by building a simple one together. More specifically, let's build a model that tries to predict the next word in a sentence, given the last word we saw. One approach to accomplish this is to simply count, for each unique word, what words tend to follow it. To do that, let's start by using the `extract_word_ngrams` tool we showed earlier, and collect all the unique words in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Vocabulary (n = 40)\n",
      "---------------------------------------\n",
      "['they', 'll', 'best', 'it', 'If', 'with', 'you', 'doctor', 'fridge', 'place', 'the', 'that', 'one', 'in', 'keeps', '’', 'a', 'away', 'put', 'also', 'an', 'have', ',', 'apple', 'coming', 'be', 'having', '!', 'for', ';', 'if', 'because', 'fish', 'You', 'cold', 'know', '.', 'say', 'day', 'well']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# some example text\n",
    "data = \"\"\"a fish keeps for a day; a fish keeps well if you put it in a cold place. \n",
    "          a fish keeps best if you put it in the fridge. If you’ll be having fish, \n",
    "          have it with an apple because one apple a day keeps the doctor away! \n",
    "          You know they also say that a day keeps coming.\"\"\"\n",
    "\n",
    "unigrams = extract_word_ngrams(data,1)\n",
    "\n",
    "# get the unique words:\n",
    "vocabulary = list(set(unigrams))\n",
    "print('---------------------------------------')\n",
    "print('Vocabulary (n = ' + str(len(vocabulary)) +')')\n",
    "print('---------------------------------------')\n",
    "print(str(vocabulary) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of the words, lets initialize a `JSON` object called `counts` that will keep track of the next word, given the current word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for given_word in vocabulary:\n",
    "    counts[given_word] = {}\n",
    "    for next_word in vocabulary:\n",
    "        counts[given_word][next_word] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `counts` object is a dictionary of dictionaries. It's convenient to store the count data this way because we can see the counts of the next word, given the current word, by simply providing the count object the current word. Let's take a look at the count of all possible next words, given that the current word is `fish`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Count of next words, given that current word is \"fish\"\n",
      "------------------------------------------------------\n",
      "{'one': 0, 'it': 0, 'put': 0, 'having': 0, 'place': 0, 'best': 0, ',': 0, 'cold': 0, 'fish': 0, 'You': 0, 'coming': 0, '!': 0, 'have': 0, 'fridge': 0, 'know': 0, 'say': 0, 'that': 0, 'If': 0, 'keeps': 0, 'you': 0, 'in': 0, '.': 0, 'with': 0, 'away': 0, ';': 0, 'll': 0, 'they': 0, 'apple': 0, 'the': 0, 'for': 0, 'if': 0, 'well': 0, 'an': 0, 'because': 0, 'doctor': 0, 'a': 0, 'also': 0, 'be': 0, '’': 0, 'day': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "print('Count of next words, given that current word is \"fish\"')\n",
    "print('------------------------------------------------------')\n",
    "print(counts['fish'])\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the counts are currently empty; that's because we've only just initialized this object and haven't actually counted anything yet! Let's do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(unigrams)-1):\n",
    "    counts[unigrams[i]][unigrams[i+1]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at that `fish` count again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Count of next words, given that current word is \"fish\"\n",
      "------------------------------------------------------\n",
      "{'one': 0, 'it': 0, 'put': 0, 'having': 0, 'place': 0, 'best': 0, ',': 1, 'cold': 0, 'fish': 0, 'You': 0, 'coming': 0, '!': 0, 'have': 0, 'fridge': 0, 'know': 0, 'say': 0, 'that': 0, 'If': 0, 'keeps': 3, 'you': 0, 'in': 0, '.': 0, 'with': 0, 'away': 0, ';': 0, 'll': 0, 'they': 0, 'apple': 0, 'the': 0, 'for': 0, 'if': 0, 'well': 0, 'an': 0, 'because': 0, 'doctor': 0, 'a': 0, 'also': 0, 'be': 0, '’': 0, 'day': 0}\n",
      "\n",
      "\n",
      "------------------------------------------------------\n",
      "Count of next word \"keeps\", given that current word is \"fish\"\n",
      "------------------------------------------------------\n",
      "3\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "print('Count of next words, given that current word is \"fish\"')\n",
    "print('------------------------------------------------------')\n",
    "print(counts['fish'])\n",
    "\n",
    "print('\\n')\n",
    "print('------------------------------------------------------')\n",
    "print('Count of next word \"keeps\", given that current word is \"fish\"')\n",
    "print('------------------------------------------------------')\n",
    "print(counts['fish']['keeps'])\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a count of the words, we can convert these to probabilities by simply dividing by the total incidence of next words, for a given current word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the probabilites JSON object\n",
    "probs = {}\n",
    "for given_word in vocabulary:\n",
    "    probs[given_word] = {}\n",
    "    for next_word in vocabulary:\n",
    "        probs[given_word][next_word] = 0\n",
    "\n",
    "# convert the counts to probabilites\n",
    "for key, value in counts.items():\n",
    "    denominator = 0\n",
    "    for key2, value2 in counts[key].items():\n",
    "        denominator += value2\n",
    "\n",
    "    for key2, value2 in counts[key].items():\n",
    "        probs[key][key2] = value2 / denominator\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the probability of the next word being `keeps`, given that the current word is `fish` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "The probability of the next word being keeps,\n",
      "given that the current word is fish:\n",
      "------------------------------------------------------\n",
      "75.0%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "print('The probability of the next word being keeps,') \n",
    "print('given that the current word is fish:')\n",
    "print('------------------------------------------------------')\n",
    "print(str(100*probs['fish']['keeps']) + '%')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it! You have built your first data-driven language model. Congratulations! Now let's get to work doing some fun things with the language model, like simulating new sentences. Let's do that by choosing a word, and then selecting the next word in accordance to the probability given by our language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_next_gram_from_language_model(probs, given_token):\n",
    "    distribution            = list(probs[given_token].values())\n",
    "    sample_from_multinomial = np.random.multinomial(1,distribution)\n",
    "    sample_index            = np.where(sample_from_multinomial==1)[0][0]\n",
    "    word_keys               = list(probs[given_token].keys())\n",
    "    next_word               = word_keys[sample_index]\n",
    "    return(next_word)\n",
    "    \n",
    "given_token = \"a\"\n",
    "next_token  = sample_next_gram_from_language_model(probs,given_token)\n",
    "\n",
    "print(' Given the token :  ' + given_token)\n",
    "print(' The next token is :  ' + next_token)\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a language model built, we can do all sorts of useful things. Including running our model to generate new sentences we've never seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "a day ; a fish keeps coming . a day ;\n"
     ]
    }
   ],
   "source": [
    "def create_new_sentence(length, seed_token):\n",
    "    tokens = [seed_token]\n",
    "    for i in range(length):\n",
    "        tokens.append(sample_next_gram_from_language_model(probs,tokens[-1]))  \n",
    "    return tokens\n",
    "      \n",
    "print('---------------------------------------')\n",
    "print(' '.join(create_new_sentence(10, 'a')))\n",
    "#print(' '.join(create_new_sentence(10, 'a')))\n",
    "#print(' '.join(create_new_sentence(10, 'a')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr> \n",
    "\n",
    "## Learning Exercise 4:\n",
    "#### Worth 2/5 Points\n",
    "Now that you have built your first language model, let's turn our attention back to the philosophers. For this final learning exercise, you will be building a tri-gram `Nietzsche language model`, and a tri-gram `Russel Language Model` using the 5 books from each of the philosophers you collected from Project Gutenburg earlier. \n",
    "\n",
    "Generate a conversation between your philosophers models by seeding one of the models with a tri-gram and allow it to run until it generates a terminating character (a '.', '?' or '!'). Then, have the other model generate a response by providing the ending tri-gram of the the first model as the seed tri-gram for the other model. Iterate like this for 15 sentences and print the text.\n",
    "\n",
    "**Note:** You probably learned from the earlier components of this assignment that Nietzsche and Russel have slightly different writing styles (yes, that was an understatement). For this reason, there are likely to be instances where the ending tri-gram from your Nietzsche model does not show up in the Russel language model and vice-versa. Use Laplace smoothing to handle this problem.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download 5 books that were authored by Bertrand Russell and 5 books that were authored by Friedrich Nietzsche\n",
    "Books_Russell = ['https://www.gutenberg.org/cache/epub/2529/pg2529.txt',\n",
    "                 'https://www.gutenberg.org/files/38280/38280-0.txt',\n",
    "                 'https://www.gutenberg.org/cache/epub/13940/pg13940.txt',\n",
    "                 'https://www.gutenberg.org/files/25447/25447-0.txt',\n",
    "                 'https://www.gutenberg.org/files/44932/44932-0.txt'  \n",
    "                ]\n",
    "\n",
    "Books_Nietzsche = ['https://www.gutenberg.org/files/52319/52319-0.txt', \n",
    "                   'https://www.gutenberg.org/files/51356/51356-0.txt',    \n",
    "                   'https://www.gutenberg.org/files/7205/7205-0.txt',  \n",
    "                   'https://www.gutenberg.org/files/52263/52263-0.txt',  \n",
    "                   'https://www.gutenberg.org/files/52190/52190-0.txt'\n",
    "                  ]  \n",
    "\n",
    "#download all books\n",
    "text_Russell = text_Nietzsche = \"\"\n",
    "\n",
    "for m in Books_Russell:\n",
    "    site = requests.get(m)\n",
    "    site.encoding = site.apparent_encoding\n",
    "    text = site.text\n",
    "    text_Russell = text_Russell + \" \" + text\n",
    "    \n",
    "for m in Books_Nietzsche:\n",
    "    site = requests.get(m)\n",
    "    site.encoding = site.apparent_encoding\n",
    "    text = site.text\n",
    "    text_Nietzsche = text_Nietzsche + \" \" + text\n",
    "\n",
    "#n-gram counts, and vocabulary\n",
    "counts_Russell, vocabulary_Russell = HW1_basicLanguageModel(text_Russell, gram_size = 3)\n",
    "counts_Russell, vocabulary_Russell = HW1_basicLanguageModel(text_Russell, gram_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# INSERT YOUR CODE HERE\n",
    "# DO NOT FORGET TO PRINT YOUR MEANINGFUL RESULTS TO THE SCREEN.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_Russell, vocabulary_Russell = HW1_basicLanguageModel(text_Russell, gram_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and clatter of',\n",
       " 'initiated to its',\n",
       " 'meaning , and',\n",
       " 'actually to write',\n",
       " \"perspective '' to\",\n",
       " 'pilot , after']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_Russell[:6] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'___NUMBLANKGRAMS___': 286058,\n",
       " 'Project Gutenberg EBook': 4,\n",
       " 'Project Gutenberg Literary': 11}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_Russell['The Project Gutenberg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE!\n",
    "#prob calculation\n",
    "  1. find which location is '___NUMBLANKGRAMS___'\n",
    "  2. remove this location from temp_count\n",
    "  3. prob\n",
    "  4. try distribution = list(probs[\"a\"].values())\n",
    "         np.random.multinomial(1,distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['___NUMBLANKGRAMS___', 'Project Gutenberg EBook', 'Project Gutenberg Literary']\n",
      "[286058, 4, 11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_denom = sum(counts_Russell['The Project Gutenberg'].values()) \n",
    "\n",
    "temp_count = list(counts_Russell['The Project Gutenberg'].values())\n",
    "temp_vocabulary = list(counts_Russell['The Project Gutenberg'].keys())\n",
    "#temp_prob =  \n",
    "\n",
    "print(temp_vocabulary)\n",
    "print(temp_count)\n",
    "temp_vocabulary.index('___NUMBLANKGRAMS___') \n",
    "#temp_denom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.999947565831099,\n",
       " 1.3982445040251964e-05,\n",
       " 3.84517238606929e-05,\n",
       " 3.495611260062991e-06,\n",
       " 3.495611260062991e-06,\n",
       " 3.495611260062991e-06]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(temp_count)): temp_count[i] = temp_count[i]/temp_denom\n",
    "for i in range(3):        temp_count.append(1/temp_denom)\n",
    "temp_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_token = \"a\"\n",
    "#next_token  = sample_next_gram_from_language_model(probs,given_token)\n",
    "\n",
    "#def sample_next_gram_from_language_model(probs, given_token):\n",
    "distribution            = list(probs[given_token].values())\n",
    "sample_from_multinomial = np.random.multinomial(1,distribution)\n",
    "sample_index            = np.where(sample_from_multinomial==1)[0][0]\n",
    "word_keys               = list(probs[given_token].keys())\n",
    "next_word               = word_keys[sample_index]\n",
    "#    return(next_word)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My modification from HW2 utils.py\n",
    "#def basicLanguageModel(data, gram_size = 1):\n",
    "\n",
    "def HW1_basicLanguageModel(data, gram_size):\n",
    "    grams  = extract_word_ngrams(data,num=gram_size) \n",
    "    counts = {}\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Count the next gram, given the current gram:\n",
    "    # -----------------------------------------------\n",
    "    possible_nextgrams = len(list(set(grams)))\n",
    "    for i in range(len(grams)-1):  \n",
    "        if grams[i] not in counts:\n",
    "            counts[grams[i]]                     = {}\n",
    "            counts[grams[i]]['___NUMBLANKGRAMS___'] = possible_nextgrams - 1\n",
    "            counts[grams[i]][grams[i+1]] = 1   #2-->1 (my modification)\n",
    "        else:\n",
    "            if grams[i+1] not in counts[grams[i]]:\n",
    "                counts[grams[i]][grams[i+1]] = 1   \n",
    "                counts[grams[i]]['___NUMBLANKGRAMS___'] -= 1 \n",
    "            else:\n",
    "                counts[grams[i]][grams[i+1]] += 1 \n",
    "\n",
    "    if grams[len(grams)-1] not in counts:\n",
    "            counts[grams[len(grams)-1]] = {} \n",
    "\n",
    "    #Laplace smooth by adding 1 to counts (my addition)\n",
    "    for key, value in counts.items():\n",
    "        for key2, value2 in counts[key].items():\n",
    "            if key2 != '___NUMBLANKGRAMS___':\n",
    "                counts[key][key2] = counts[key][key2] + 1\n",
    "    # -----------------------------------------------\n",
    "    # convert the counts to probabilites and Laplacian Smooth\n",
    "    # -----------------------------------------------\n",
    "    #probs = counts\n",
    "    #for key, value in counts.items():\n",
    "    #    denominator = 0\n",
    "    #    total_prob  = 0\n",
    "    #    for key2, value2 in counts[key].items():      \n",
    "    #        denominator += value2\n",
    "\n",
    "    #    for key2, value2 in counts[key].items():\n",
    "    #        if key2 != '___NUMBLANKGRAMS___':\n",
    "    #            probs[key][key2] = value2 / denominator\n",
    "    #            total_prob      += probs[key][key2]\n",
    "\n",
    "    #    if '___NUMBLANKGRAMS___' not in counts[key]:\n",
    "    #        counts[key]['___NUMBLANKGRAMS___'] = possible_nextgrams\n",
    "    #        counts[key]['___BLANKUNITPROB___'] = 1/(possible_nextgrams)   \n",
    "    #    elif counts[key]['___NUMBLANKGRAMS___'] != 0: \n",
    "    #        probs[key]['___BLANKUNITPROB___'] = (1-total_prob)/counts[key]['___NUMBLANKGRAMS___']\n",
    "    #    else:\n",
    "    #        probs[key]['___BLANKUNITPROB___'] = 0 #??\n",
    "        \n",
    "        # -----------------------------------------------\n",
    "        # Obtain the prior probabilities for each gram:\n",
    "        # -----------------------------------------------\n",
    "     #   gram_counts = CountFrequency(grams)\n",
    "     #   count_total = sum(gram_counts.values())\n",
    "     #   for key, value in gram_counts.items():\n",
    "     #       probs[key]['___PRIOR___'] = value/count_total\n",
    "\n",
    "     #   probs['___TOTALTOKENS___']  = len(grams)\n",
    "     #   probs['___UNIQUETOKENS___'] = len(set(grams))\n",
    "        \n",
    "        return counts, list(set(grams))  #probs\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a backoff model:\n",
    "def trainBackoffModel(corpora, max_gram_size):\n",
    "    langauge_model = []\n",
    "    for gram_size in range(1,max_gram_size+1):\n",
    "        langauge_model.append(basicLanguageModel(corpora, gram_size = gram_size))\n",
    "    return langauge_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#HERE!\n",
    "\n",
    "# Begin by extracting a set of unigrams \n",
    "segments      = extract_word_ngrams(data, 1)\n",
    "max_gram_size = min([len(segments), len(model)])\n",
    "\n",
    "segment_start = 0\n",
    "initial       = True\n",
    "log_prob      = 0\n",
    "complete      = False\n",
    "\n",
    "# Ideally, we would like to use the entire given segment, but if we can't find it\n",
    "# In the dictionary, we will back out, and check if we can find a match for a smaller\n",
    "# n-gram.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> INSERT AN INTERPRETATION OF YOUR RESULTS HERE </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><span style=\"color:red\"> Self Assessment </span></h1>\n",
    "Please provide an assessment of how successfully you accomplished the learning exercises in this assignment according to the instruction provided; do not assign yourself points for effort. This self assessment will be used as a starting point when I grade your assignments. Please note that if you over-estimate your grade on a given learning exercise, you will face a 50% penalty on the total points granted for that exercise. If you underestimate your grade, there will be no penalty.\n",
    "\n",
    "* Learning Exercise 1: \n",
    "    * <span style=\"color:red\">X</span>/1 points\n",
    "* Learning Exercise 2: \n",
    "    * <span style=\"color:red\">X</span>/1 points\n",
    "* Learning Exercise 3:\n",
    "    * <span style=\"color:red\">X</span>/1 points\n",
    "* Learning Exercise 4:\n",
    "    * <span style=\"color:red\">X</span>/2 points\n",
    "\n",
    "#### Total Grade: \n",
    "<span style=\"color:red\">X</span>/5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
